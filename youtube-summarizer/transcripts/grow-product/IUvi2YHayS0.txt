PMS are designing AI products completely wrong. >> Designing with AI isn't about prompting. It's about understanding the entire workflow, the system, the constraints, and the behaviors. >> Genron is using Google AI studio and stitch to create [music] full-fledged prototypes that are actually designed well. >> I can make him very [music] refined or Yulo which is going crazy. You know, everyone can rely on AI to generate something. But again, AI is not human, right? it does not really have the enough empathy and that's why this GPT is meant for the very first prompt to AI prototyping >> and you didn't use claude for the prompt you prefer Chad GBT for the prompt I like to shift it to CHBT in order to save tokens for claude [laughter] there are also other tools like uh subframe magic [music] patterns but they are more specific I know many people are always facing the blank [music] canvas question I wanted to design something out of AI but I'm not really clear about what I wanted to be dying for. >> Wait, show me what that really means. Before we go any further, do me a favor and check that you are subscribed on YouTube and following on Apple and Spotify podcasts. And if you want to get access to amazing AI tools, check out my bundle where if you become an annual subscriber to my newsletter, you get a full year free of the paid plans of Mobin, Arise, Relay app, Dovetail, Linear, Magic Patterns, Deep Sky, Reforge, Build, Descript, and Speechify. So, be sure to check that out at bundle.acashg.com. And now into today's episode, Ginron, welcome to the podcast. >> Thank you so much for having me. >> My pleasure. I've been following you online and I think you are one of the world's leading experts on design with AI, the title of your newsletter, and I'm really excited for you to bring this knowledge to everybody. And I think one of the first things we need to do is we need to build the neural circuitry to understand what is designed with AI. We need to build the neuronal connections. We need to build a mind map. So can you explain to us what is design with AI? What is the universe of design with AI that people should know? Design with AI covers a lot of things. You know design and AI are very broad topics. So to make things easier to understand, I actually did a mind map. Let me show you. First of all, there is a part of design with AI which is about prompting like how can we prompt better in order to get better results. There are also about tactical things like how can we ideuliate with AI better using AI as assistant to generate better ideas that human is hard to to think think about. Not to mention there are also other design areas such as design and prototyping with the help of AI and there are some workflows that I can explain more. Last but not least is a part of the landscape that fewer people talk about. That is aside from those tactics about prompting, designing, ideulating and prototyping. There's also a part of a world where we need to design, we need to stay conscious designing with AI and staying conscious mean how can we bring more intentions and thoughtfulness into the things that we are designing and that also involves in the awareness of different kind of risks and how to m mitigate those risks. So that's just a a broad overview of the items and I can walk through each of the section really quickly and if you have any any questions Akash feel free to ask me. >> Yeah let's start at the top prompting with AI. I guess it's kind of a universal skill. A lot of people might kind of roll their eyes like I've seen so many influencers post prompting frameworks. What do they need to know about prompting with AI to use these AI design tools? Well, >> cool. Let me zoom in a little bit because it's easy to to get distracted when you see a lot of things. So let's start with prompt with AI. In my opinion, I've got to see many prompting frameworks and I believe Akash that you've seen over the years as well. There are so many of the frameworks out there but in my opinion to its core prompting with AI it's just a way of how human interact with AI in order to get better results and I try to make things as simple as possible in from a design perspective as opposed to you know introducing so many frameworks that people can choose. So from that regard, I jotted down some of the areas that I think are essential to prompt with AI to get better design results. The very first one is clarifying the ask before we ever even engage with AI. It's important to know what we want to get out of AI. So you can see clarifying the ask as request. What you want to get out of AI? What do you want to include? What do you want to avoid? And if you don't know those things, which is okay because often time it depends on the scenarios. We just don't know what we want and that is okay. We can leverage people for help. We can even ask AI for help to gain some clarity. So those are really the small things around clarifying the ask. Context is also very important. And I believe Akash you've seen this trend of context engineering becomes more trendy terms versus prompt prompt prompt engineering. It's just because of the fact that people realize that how important context can be and it can amplify the effectiveness of prompt. And regarding context, first of all, I wanted to call out that not more context means better, but the necessary context that is related to the go. I think those are the things around context that really matters. That's why as you can see at the very first part I mentioned about providing context is really about clarifying the outcome so that the clarity can help inform what to provide for the context and only providing the necessary context that involves what row are you in what are the unmet user needs what timeline are you facing basically constraints right time constraint technical constraints is there any existing ideas that AI needs to know because if you have many ideas already you might want AI to know right It really depends on your perspective. Some people wanted to hide some ideas from AI so AI can come up some fresh ideas, but some people want AI to have something to base off from as opposed to, you know, drifting too far. So, it's really about that what is your prioritization criteria. AI struggles with priority because it does not really know what you want. So, if you have something to prioritize and to focus on, that information could be really helpful. Who is the audience? Audience matters when you're trying to frame experience for a particular set of users and that really helps if you can provide the audience information. What are the design principles and what are the brand guidelines? Those are all important constraints that you can provide to AI depends on different scenarios. There are some miscellaneous points like be aware of the existing memory uh refresh certain memory by open up a new chat but those are just tactical things. Providing references also helps because we have learned that when providing references whether it's text reference output format visual references code references AI tends to give you better design outcome. There are some extra tips around prompting such as structure in structure out simplicity. It's okay to be imperfect reverse prompting and so on so forth. How much should you define the exact design you want? because of course AI is going to do a much better job at giving you what you want if you're really specific but at the same time then you kind of lose that exploratory lens and magic and part of the magic is you can get three to four divergent designs so I think we'll have to explore that as part of one of the workflows but when you think about prompting do you feel like you need to prompt these design tools with a very structured mega prompt or you can just do like the shorter kind of to the point prompts >> I think it's an art In some ways by saying art meaning sometimes we want more control of the outcome and sometimes we want to give AI more space to brainstorm and it's really a fine balance is as if you are hiring someone right sometimes you want to be very specific and sometimes you wanted to give her or him a vast amount of creativity or space to to brainstorm. So in my based on my experience, it really depends regardless. I think some of the structure helps if you can introduce it relatively early. That often helps as opposed to you know starting everything's from scratch. You know it is okay to do things that way but it also requires a lot of time and tokens for sure. >> Awesome. So maybe we can go into the details of it as we think about ideiation with AI which I think was one of your next points. >> Sure. Yeah. Ideation ideating with AI. I actually have two parts which is ideulating with AI design and prototyping with AI. They really go hand inand as opposed to you know this is ideation this is design this is prototyping right given AI has bring some convergence to those those things. But just wanted to give some ideas around those early stage ideation such as brainstorming idea right let's say akash we are thinking of a fresh product ideas we need some early stage brainstorming such as you know providing necessary business goals and problems user goals and problem just to provide certain context or guard rails and you know relevant user insights relevant constraints and those are the things that we can provide for some early product idea brainstorming and regarding convergent thinking is also important because AI can get really really wide and extensive and that's why we also need things to to converge a bit for example ask for rent for ranking ideas or ask for examples for better evaluation like you know sometimes you ask AI AI give you a lot of ideas but you don't know where that idea come from so it's basically asking you like where can you provide me the examples and where did the examples come from? What are the sources? So, you can use those to do your due diligence if AI is just making things up or you think that the sources that AI provide ended up being credible, right? Just for your for your own sake, for better evaluation. So, that's that. Regarding design and prototyping with AI, it's more like an extension of ideating with AI. So, that's why I said they go hand in hand. There are some common best practices, right? specifying instruction, specifying the context, brainstorming design viations, keep track of design vibrations, navigate between options. So, Akash as you can see those areas like the first, let's say the second one and third one, it can be reflected in prompting, right? The very first one can be indirectly reflected in prompting, but the fourth, fifth and sixth ones are become more ambiguous. It's more like how you operate within the design landscape versus mere prompting and those of the things were a challenge like years ago but as more and more AI tools realized this painoint they started to have new features that addressed those pain points that is easier for designers or non-designers to design I would say so those are some principles and pra best common practices I would say around design with AI and of Of course there are workflows. Workflows I put into two big buckets. One is more like from an idea to design and often time it is leaning more heavily on text or ideas versus actual existing designs. And I included here you can clarify the ideas through different AI tools such as CHBT cloud and Gemini or I will mention a custom GPT that I've built in a bit. So it's essentially a type of chatbt. It's just that I provided some extra thoughts to it to save people's time and to save my time. Regarding AI tools, I can explain further about the nuances between different AI tools. So that those are not treated equally and it really depends on what you want to get out of them. Regarding from an existing experience to design, well that actually requires more guarder rails around what you want to do, right? It may be you have existing website that you can take snapshots from or you have existing Figma file because Figma is the go-to design tools by many designers. Um right so there are different flow workflows I can talk about in this area as well. Today's episode is brought to you by NI1. In tech buying speed is survival. How fast you can get a product in front of customers decides if you will win. If it takes you 9 months to buy one piece of tech you're dead in the water right now. Financial services are under pressure to get AI live, but in a regulated industry, the roadblocks are real. NI1 changes that. Their airgapped cloud agnostic sandbox lets you find, test, and validate new AI tools much faster from months to weeks, from stuck to shift. If you're ready to accelerate AI adoption, check out NIA1 at nia1.com/acos. That's n a y ao n.com/ a kas. Today's podcast is brought to you by Pendo, the leading software experience management platform. McKenzie found that 78% of companies are using Genai, but just as many have reported no bottom line improvements. So, how do you know if your AI agents are actually working? Are they giving users the wrong answers, creating more work instead of less, improving retention, or hurting it? When your software data and AI data are disconnected, you can't answer these questions. But when you bring all your usage data together in one place, you can see what users do before, during, and after they use AI, showing you when agents work, how they help you grow, and when to prioritize on your roadmap. Pendo Agent Analytics is the only solution built to do this for product teams. Start measuring your AI's performance with agent analytics at pendo.io/acos. That's pendo.io aka. How many designers are using the Figma make to Figma workflow? >> Yeah, great question. So, I've talked with many designers. The design the product designers in the states are using Figma make quite a bit just because the fact that they have the Figma paid plans. That's why they have access to Figma make already. But when I talk with people outside of the US, for example, I talked with another designer who's based in uh Asia and he told me that they not a lot of people are you were use are using Figma make and I was surprised because they do have access to Figma. I don't know why and for Europe some of them and I know some of them they just don't have access to Figma make for like whatever reasons. So it seems like if designers get access they are using it quite a bit. So that's really interesting the pickup on these AI prototyping tools not just for PMs but clearly for designers as well. What's that fourth area you had about being conscious with AI? Can you give us the next layer deeper on it? >> Sure. The being conscious with AI it's more about aside from all the tactics of how can we use AI to design better smarter and faster. I do wanted to bring another layer of things. You know, I may not have a lot of time to cover this today, but I believe that for during some of the workflows, this is already indirectly interjected in some of my demos. It's basically how to bring more intentions and thoughts around what you're generating as opposed to fully 100% being driven by AI. I still think some control um as a human or some intentionality can go a long way. So for example some awareness about risks you know like I just mentioned about the early brainstorming with AI product ideas right we need to be aware that there are some hallucination you know biased insights outdated insights irrelevant insights or lowquality generic insights that can be part of the insights that we subconsciously have brought in like for example let's say I'm designing for something let's say designing for whatever experience like you know restaurant booking experience experience, right? If I were just to ask Chad GBT what's the main pain point of the users, it can give me very like general insights or even biased insights that are tailored around a specific type of users, right? So that could be dangerous or they just make things up or they provide generic insights that not really helpful. So that's that. Regarding recommendations are just ways to mitigate your risks like for example you know keep human in the loop during the research ideation process double check the sources empathy with the people that you're designing for becomes even more important because you know everyone can rely on AI to generate something but again AI is not human right it does not really have the enough empathy with the people who are you they don't really care to certain extent including diverse perspectives in the process become very important again This is also about design intentionality and this is a huge deal uh for for product designers giving user control over automated decision and important data. I think this is pretty self-evident audit for AI outputs uh nuanced information matters near transcript. So this is also another interesting point I want to call out. It's just because of fact that I noticed a lot of um PMs and including designers they would fetch different information they would feed information into different AI tools and they get certain insights and then they just build and design on top of those insights. But again there are so much to it. For example, the behavioral insights or sarcasism or hesitation, right? Those kind of behavior coups that can it's really hard for AI to understand with mere transcripts. So just a way to call out that you know before we actually generating something out of AI just make sure the insights are high quality. >> Makes sense. So if you had to pick two workflows to demo for folks, what are the things that people need to walk away and learn from this video? >> There are two things I can demo one of the workflow which is around this area that is from a blue sky idea more or less to design because some people are working in this area. I can also give another workflow which is about from existing experience to design. I can give a demo of something like both of the workflows that I will show today are not very commonly used by people and I thought maybe that can also bring some fresh perspectives. >> All right, let's do it. And promise me you're going to show me all of the details, right? Not hide anything from us. >> Of course. Of course. And actually, as a matter of fact, I would be surprised if there's no bugs occurred during a live AI demo. That's just the nature of reality of things and I wanted to show if in case that happens to you all as well. >> All right, let's do workflow one. >> So the workflow I'm going to explain is more about custom GPTs to PRD for prototyping and a lot of AI tools. I know this slide can be confusing. What does really mean? Let me explain a little bit. So first of all, I have built a custom GPT. a custom GPT. You can consider that as a chat GPT but with my personal instructions and I will show you how I follow the steps within that custom GPTs to generate a effective quote unquote PRD for prototyping. So it's not a broad PRD for everything. It's only for AI prototyping. So then I can copy paste that PRD for prototyping into any of the AI tools that you want. Although as I mentioned earlier there are some difference between different AI tools but for now you don't have to worry too much about that area. We can just focus on this area first and during the demo I'll also you know paste into a couple of tools to give you a sense of how it how it works. Does that sound good to you Akash? >> Yeah maybe you can show us like if somebody was building this custom GPT from scratch how they would build it. >> Cool. I think that will also touch upon some areas. Um, do you want me to show you how to build a custom GPT from scratch or more about the frameworks how I can better build a >> how to build this one, right? Because I guess this custom GPT is about PRDS for prototyping tools. So, maybe we can just kind of show people like these are the inputs I would put into the custom GPT generator. >> Sure. Sure. Um I can give you some of the general guidelines as I mentioned earlier right effective prototypes is about request what you want to get out of AI the context and reference right those are very broad items I can be a little bit more specific here so in this particular scenario is that I don't know Akash but I know many people are always facing the blank canvas question I wanted to design something out of AI but I'm not really clear about what what I wanted to design for that's why I have to vibe coding quote unquote right one sentence at a time but sometimes it can easily go into loops which is frustrating because you're not clear in the first place and that's why I started to build this custom GPT because I wanted to bring myself some clarity around what I need to put into the prompt >> and that's be that is tied to the context for example who am I designing for right that is important and sometimes when you're writing things in AI tools this is something can be neglected. What are their needs? This is also a question that I put in the custom instruction of the custom GPT just to give me some thoughts to think about. What are we trying to what are they trying to achieve and this is about user goals, right? And which experience do you want to focus on? This is a interesting one because often time we focus on too many things and AI struggles if we just wanted to build a full stack app from scratch and that's why I want to bring myself or bring others some clarity around which specific area whether it's the platform or the user flow that you want to prioritize building first. And once that gets figured out, it's much easier for you to scale up. For example, if the mobile web solution is good, then it's much easier for you to scale it up or to apply to a another type of experience. And let's say if the core user flow is figured out, then the login logout experience become so easy because it's not addressing the core user needs. It's pretty standard elsewhere. That's why I try to be hyperfocused for the very first prompt and very first prompt matters and I I believe Akashi have some similar experience that the very first prompt will set the stage for a lot of things and that's why this GPT is meant for the very first prompt to AI prototyping. >> Got it. So you load up a go to custom GPT generator you load it up with this information. This is what an effective versus a bad prompt is and this is the context questions I want you to answer. Make sense? >> Yes. And again, this is just one type of the custom GPTs. You can build your own custom GPTs based on your needs. So this is more of a simplified version of quickly generate something or quickly generating a effective prototype that matches your idea and you can build more you know more complex things over over it. >> Yeah. Cool. All right. So, I can give a demo. So, let me drag this over here. And I can click here to start. Again, this is also what I built in the custom GBT as well. So, just make it easier for people to on board. All right. Great. Great. Let's get started. As you can see, I broke questions into different smaller chunks that is easier for user to control. So, what is the main goal of your product? For the sake of the demo, I would just go with whatever comes, you know, shows here by default. You know, if we have time towards the end, we can I'm I'm happy to go crazy like we can come up with something from scratch. But for the for the sake of demo, let's just go easy with whatever suggested here. So, let's say help user track expenses, right? That's the project goal or what user want to do. What are the intended user of the product? So, let's say um freelancers. >> Okay. So, you built this custom GPT in sort of a way where it's extracting the information from you before it gives you the prompt, >> right? It's like asking me some questions in order for it to better perform the task. >> Yeah. >> Cool. Uh, third question, what platform is your product for? And this is also something special about the custom GBT is that I make it really specific as you can tell here. So, in this case, I can just type a number like let's say one. All right. A responsive. Okay. So those are the key user flows that help users achieve the goal. As you can see here Akash that I it does not generate login or it does not recommend or provide log in and log out as a option and I did that on purpose. It is specifically called out in the system instruction is that I do not want that to be a part of recommendation. The reason is that if you're just using some other CHBT like using other custom GPTs or the general CHGBT chances are the first thing it will recommend you do is to include the sign up sign in experience but to me that is not the key user flow that we need to focus on first right it's not directly solving the user problem that's why see it generated some ideas for you so I can say something like you know one and um maybe part of two. It really depends. I can say one, I can say two, I can say three, I can say all of them. Again, I try to keep things simple. All right. So, here's a preview of your design spec draft. This is a preview and I can always check, you know, if there's any issue or something like that. But as you can tell, this is a very lightweight PRD. And I don't really call it a PRD because it's more like a spec a spec for AI tools to generate a prototype. So the focus is really on front end presentations such as components and interactions. Right? Let me scroll down like do you get to see those kind of like backend logics or technology? You don't just because of I specifically ask it to not include those things so that you know AI can be more focused to generate the the front-end experience first. So that's that and I always encourage people to make some revisions and double check if that makes sense as opposed to just blindly rely on what AI has generated. All right, this preview looks good to you. And this is also a checking point that I built in the system instruction, right? Check if does it look good to you? If not, make revisions. If yes, it will generate the final markdown. So let's let's say yes. This is the markdown format in a code block and I specifically asked for this in the back end because it just make things easier for me as opposed to first as opposed to copy paste into any other AI prototyping tools and also I made it easier for me because it's a markdown format. it can retain better retain the hierarchy of the spec so that it can make AI easier to understand in order to get better results. >> Yeah, I see you're kind of doing two things here, right? You're not making this too long so that you experience context rot but at the same time you are being very very specific about these are the four screens I want to see. This is the key user flow. So you're defining the front end quite specifically uh but nothing else >> right I try to find a b balance and I know some people prefer a little bit more details some people some people prefer less details but this is more like a sweet spot that I found for a lot of my use cases again you know other people can customize from there cool all right so let's say we're happy with this right I'm going to paste into any of the AI popular AI prototyping tools Let me start with claude or you can call it claude.ai or claude artifact. Right? The reason I like about claude is not because it can generate the best quality of design. It's more about it's a well-rounded tool that can give me a quick mock run of what I can expect from this prompt. Right? If I were to ask you Akash if this makes sense, it's hard for you to visualize things, right? It's hard to tell just because there's so much information. So, I like to use claw.ai as a lightweight mockron tool to double check if the prompt makes sense or if it's running into any errors or something I'm not aware of. If so, I can always go back to the drawing board to revise. >> Got it. And you didn't use Claude for the prompt. You prefer Jad GBD for the prompt. >> Yes. Uh, great question. The reason I do this is that just because of the fact that I know claude can be very powerful in terms of code related tasks. That's why I like to offset things are not code related in Chacht or Gemini. For example, clarity related stuff. I like to shift it to CHBT in order to save tokens for Claude. [laughter] >> Yes, the Claude tokens. I'm on the 20x max plan right now. >> You're paid. >> That's incredible. So that's not a that's not a problem for you then. >> Well, I still hit the limits. I push Cloud Code to the limit every day. [laughter] >> Uh it's funny because I Yes. Like I hit the limit as well a couple of times, but I'm not on the max plan though. >> If you've been enjoying today's episode, you are going to love Genron's course AI for product designers. It is on Maven. It has 4.6 six stars with 116 reviews. It is a twoeek intensive live boot camp where you're not just going to learn what we showed in this episode, but the next layer deeper. You're going to learn from all of his expertise in trying out all these different tools. Hundreds of other students have taken his course so that they can get better at product design. We're all feeling that anxiety about AI. If you want to become better at product design, whether you're a PM, product designer, or engineer, check out his course AI for product designers on Maven. You get a discount by using my code, Akos X-Maven. It's $100. We might even update it to more. So, go get that code, use my link, check out his course. And now, back to today's episode. Here's the dirty secret about prototyping. You spend two weeks building a prototype. You validate your assumptions. Engineering loves the direction. Then what happens? You throw the whole thing away. Bolt changes this completely. When you prototype in Bolt, you're not building throwaway mockup. You're building real front-end code that integrates with your existing design system. So, when you hand it to engineering, they don't throw it away. They ship on top of what you've built. I use Bolt every single day. I host my land PM job cohort on it. And honestly, I'm up till 2:00 a.m. some days just vibing in the tool, having fun, and building. That's when you know a product is good. When you're using it past midnight, not because you need to, but because you want to. Check out Bold at bolt.new/ aos. That's bt.new/ a kash. Link in the show notes. You know what kills momentum? spending 30 minutes on a brainstorm with Claude and then copy pasting it into a slide tool to create the slides. That loop is dead. Gamma now works inside Claude through Claude connectors. So you can go from thinking to a finished presentation without ever leaving the conversation. Here's what that actually looks like. You're brainstorming a product strategy in Claude. Midcon conversation, you say, "Turn this into a deck." Gamma generates it right there. You keep refining your thinking and the presentation updates with you. No copying, no pasting, no starting over in another tool. And it gets better. Connect Gmail. notion or GitHub alongside Gamma and Claude will use those connections to pull live data straight into your slides. Weekly business reviews, feature PRDs, all generated from the data you're already using. Your deck finally keeps up with your ideas. Try it now. Connect Gamma to Claude at gamma.app. Go from thinking to presenting in one flow. Claude helps you think. Gamma helps you show it. Okay. [laughter] Yeah, it's uh it's an addiction at this point. Claude tokens. >> Yeah, indeed. So I guess theoretically you could just like you architected the custom GPT you could build a claude scale or a claude project to build the prompt if somebody wanted to. The the keys when they're building that prompt generator I think we saw we're like make sure you get absolute clarity on the front end. Make sure that the user kind of agrees with it and then make sure it just generates a very simple prompt that's not going to be like over stuffed with random details like success metrics or this is the user group. Like it's really focused on defining the front end >> 100%. because it's very easy for you to put a lot of details into the prompt like irrelevant details and as I mentioned earlier it's very important to only provide the necessary context that is relevant to your goal. So I I try to be very careful about here and as you back to what you said yes you can build a cloud project something like that you can also use gem by gemini but to me gem by gemini is more like a simplified version of custom GBT based on what you can do in the back end >> but it's also free which is good for J. >> Oh [laughter] there you go. All right. So, let's let's double check what it can do, you know, because as I mentioned earlier for Claude, I don't expect something like really great, you know, it's really really simple. >> Yeah. One thing that immediately sticks out to me is the the purple, which is kind of the infamous mark of designing with AI. >> Oh, that's that's funny. Yeah. Yeah. Like, you know, this is off the off the screen. It has some essay. Let's play with it. date let's say December 19th with uh whatever you know Thai food which I had this noon jealous [laughter] >> well it's very heavy so I'm still digesting you got a curry >> I yeah I got a lot of tofu pie thai >> nice but it's all also a little bit overpriced expense view summary Yeah. >> Wow. >> I mean, that's the coolest thing about these tools is you got three or four screens in a product in minutes. >> Yeah. So, as you can see, let me let me change the the screen size a little bit so you get a better preview of what it looks like. I have not checked the spec ironically because usually I check it. But again, this is a visual representation of what I provided to it. You know, the key flows, adding new expense, see updated summary, something like that. So at least it give me a visual representation of what this prompt will entail. In other words, if I were to feed this into other AI prototyping tools that are more robust than cloud.AI, chances are I might get a similar experience. In other words, if I see something dramatically off here, maybe it's because of the prompt so that I can go back to the drawing boards to make revisions. >> Okay, so we'll assume the prompt is correct. What's the next step? By the way, usually this is already the final step if you are comfortable with prototyping claw.ai, which I believe not the case for most people. So I like to do a mock run as I mentioned to double check if the prompt makes sense. And for the next step likely, right? I don't think this people want to end at clot.ai. So you can go to any of the AI prototyping tools that out there. So let me open up Larable. So I I know some PMs like to use Larble. So let me copy paste the same prompt and uh let me paste it in Larable. >> And across all the tools that you've tried like how would you describe the pros and cons and your favorite one? >> So wow there there's a lot. Let's start with the lovable vzero boat because those three are were established relatively early around AI prototyping and in my opinions they are similar. I know lovable has they has built a lot of things over the past six months. So if you're looking for a higher quality design with a very it has a particular vibe around those design styles. Some people like it, some people are not. But if you're looking for a well-rounded tool that can deliver a good design results and you don't mind paying extra for the well-rounded experience, for the extra features, then Loverable is the one. For Vzero, I think qualitywise is similar with Loverable. I also like the fact that you can addit the code without upgrading to the paid plan, which is more accessible in a lot of ways for VZero. But again, I just have honor the impression that, you know, Larbo has been really updating a lot of things and some people prefer Vzero's design aesthetics over Laro. It's really a it's really a personal choice. For Loverable's design vibe is more like very glamorous, you know, very vibrant. So, that's that's the difference. But overall, they're they're quite similar. The LoverB may have some other bells and whistles. Regarding both, I'd no longer use as much. It's just because of the fact that Vzero and Loverable are doing quite well. But if you are interested in a like full stack prototype then I do find boat has more better integrations because of the the team behind boat. So that's Laro Vzero and boat. There are some other tools like as well like Google AI studio is something relatively new in terms of its prototyping capabilities. Google AI Studio has been around for a long time, but it just recently it's really shifted it focus from, you know, a development developer based tools to like a vibe coding tools that everyone can use. And I found it interesting. It's having some of the new features. It's free, which is good for more accessible for many people. But regarding the design quality, I feel like it's still catching up with the tools like Lover and Vzero. There are also other tools like uh subframe magic patterns but they are more little bit more specific. There are similar with well magic patterns is similar with loable per se similar with vzero but it has some actual features that are around product design and magic patterns explicitly does not have a backend as you can see that's that's a difference that's like a deliberate business decision that they've made. There's also pros and cons depending on what you want to get out of it. So this is the u the level one. At least we don't get the purple. Lovable has put into their system prompt. Avoid that purple gradient because opus 4.5 overuses that or maybe it's not using opus 4.5 on the back end. We don't know. This is a simple enough app. >> Yeah, this is also something a new trend I've seen in AI tools such as Largo or cursor is that they're not just dependent on one specific AI model. they have more than one models from one companies to leverage so that they can get a little bit smarter and less dependent on those you know AI model companies. >> Yeah, you mentioned by the way magic patterns. Magic patterns is totally free for paid subscribers of my newsletter. They get a year free of that as well as reforge build which is another one. I think both of those are really specific for like front-end PM design prototypes like you said like a bolt or a replet is much more powerful if you want to build a full stack, >> right? Yeah, we didn't mention rapid exactly. Especially you wanted to build a full stack prototype. I found rapid has a lot of things that can get you hooked up to the back end. Again, that also comes at a cost. That means that you have to pay extra for that holistic experience in >> Raplet. So, what do we take away from this lovable prototype? >> The loable prototype as you can see in ter from a detail standpoint, there are small areas that are more upgraded compared to the cloud.AI AI experience, right? Um, you know, just like random things. Save expense slightly. There's an arrow state as you can tell. Um, the drop down the color, it's more refined in a lot of ways. And save expense. There's, you know, the confirmation page. >> It's a lot better than quad. Yeah, >> as you can see, yeah, they they've spent a lot of work in terms of the the how do you say I refer to it as sub AI agents between the LLM model layer to the uh to actual presentation layer. So there's something unique about different AI tools or system prompts in other in other words. >> Yeah. It's not just a claw dropper anymore. >> Exactly. The actual layers actually helps. >> So I think another thing that designers probably care a lot about is lovable just shipped visual editing. Do you use that a lot? >> Not that much in the sense that it is helpful to a certain degree and it's still in my opinion is still catching up with this similar feature in uh Vzero and Figma make. So it's not that robust in my opinion but it's getting better and better. >> All right. So shall we move into workflow two? What's the second workflow people need to understand to really understand design with AI? >> Sure. Um so the second workflow it's more about new combo combination of tools that are Google stitch and Google stitch and Google AI studio. Um, this is very new by the way because I know some people know about Google AI Studio, but over the past I think about two months ago, Google AI Studio announced its new vibe coding whatever engine that you get to build things, build prototypes within Google AI Studio, which is exciting news. However, at this point, Google AI studio still feels like, you know, like a loverable that you get to see the code, you get to using you get to use prompt to generate a prototype, right? But it's lacking something that is the early stage design explorations and not a lot of AI tools out there can empower that. Stitch is a good example. So, stitch is a Google product. the about July, Google acquired Galileo AI and rebranded as Stitch. And over the past two months or one month specifically, Google Stitch has been actively updating its different features and in a very fast speed. Like every week there are there were there are new updates and Stitch is a tool that can help everyone easily generate design ideas. So you can see the stitch Google AI studio combo as you know getting the best of both world. Google AI studio is more like for you know prototyping interactions whereas stitch is more of early stage ideation and I found there are some interesting synergy between the two and that's why I wanted to showcase to you all. And by the way, you can do similar things in magic patterns, which I'm more familiar with, by the way, uh than stitch. But just given how new things are, um I wanted to give you all overview and hope that can be helpful. >> Cool. Yeah, I haven't seen much about Google Stitch yet. Excited. >> It's very new. All right. So, let me pull out a Google Stitch. So first of all as you can see right here is that we can actually paste directly copy paste the prompt in my Jeff GBT to here you know I can copy paste it here. This is one use case of using stitch is that you generate a set of designs from an idea similar with what we did for lovable right but for this case I wanted to try something special. So in this case uh let me show you something. Um if I go to redfin.com if I go to any of the sections I'm trying to find example that is easier for people to get trying to see if there is a there is a ask refrience. So let's say we are finding a house in Seattle. I wanted to showcase you a particular example that we can leverage stitch to to play with. Oh, we're actually in AB test as well right here. Never mind. Um, I can grab another snapshot. I think it's easier for you to people to understand. So, right here, let me grab something. Do you see this? This is a snapshot of a home detail page for a real estate platform. And there's one section called Ask Red Fin. And this is the experience of so-called AI chat for people to understand more about the home. Let's say we are designing the experience for this specific area. What could be the other design varants that we can get out of this existing experience? And I wanted to bring this up is because this sounds like a totally different challenge design challenge than the one we talked about earlier, right? So that just brings some brings some diversity to our workflows. So if I were to go back to stitch. So let's say I copy paste the design existing scenarios here and I can describe the design and for the sake of time right before this call I actually wrote down some of things as a initial prompt so that I can save my time um typing here. So, as you can see, I put it in some of the guard rails around what I want it to generate. Number one is contacts. You are reviewing the AI chat section on a home detail page of a real estate website. When potential home buyers browse a listing and interact with the section, an AI chat window will pop up. So, those are just context. The business goal is this. The user goal is that, and this is my ask. evaluate the existing experience, identify actionable improvements and generate better design ideas accordingly for the ask relevant section. I don't usually do things like that is because I'm combining several asks into one. I could have just, you know, break it down into steps. But for the sake of demo, um, let me do this just for the sake of simplicity. And something interesting about stitch is that you can toggle between app and web. Um, so in this case, let's start with web. And under here is different models. It's funny because a week ago there are only three models here and then there's a thinking with three pro pro. So how about let's try out this. This is very new by the way. And what I'm doing now again as a recap is to generate design ideas based off an snapshot of existing design with along with some of my context and requests. >> So this can be your on the divergent path when you're exploring divergent solutions. This can be your thought partner. >> Correct. If I were to go to um the diagram again, you can use prompts one idea at a time or you can generate multiple ideas. And I can explain this further. Akash. So there is an option or feature in stitch where you can generate multiple options. So it's you can stay with this mode or you can switch to a more divergent mode for design brainstorming. >> Cool. Let me close out lovable and probably I can close out those two as well and uh just wait a wait a bit for the things to do. All right. So as you can see by default it generate two options. I can actually specify you know I can make it more. I can generate three options here. So let's take a look of what it generated based on what I what I ask. There are a lot of things like as you can see you know there is some still some randomness in the outcome regarding the area outside of the ask refin section but I'm I don't care about those section as much so we can ignore it. So let's look at this section ask redin you know compared to the existing design it actually saved up some space for the prompt window prompt box which is you know arguably more clear in this way and there's also more context around this area. So that's something that's just something as a you know like um like an idea to think about. What about the other idea? This idea if what we were to compare with the original side by side I think let me make that larger if that's easier for you to see. I can I can do this big. All right. So if you were to compare with that one it's closer, right? It's similar but it's just a matter of like spacing and layout. All right. So we have something very preliminary and those two options help. But let's say we are Oh, before I go further, I want to take a pause. Any questions you have here? >> I guess they're somewhat divergent. Should we try to ask it to give even more? >> Yeah, sure. So, let's talk about this area like this option. So, let's say we are interested in exploring this area more. You know, I could have taken another snapshot, but let's just start from there. I can click here, drop down, and there is a regenerate feature where I can rerun it again if I'm not happy with what I see right here, which is always a little trick for generate more ideas. But I can also do varation, which I use it more often. So let's say for So first of all, I can define how many options I can generate. So I can make it four, I can make it two, but let's just make three as the default for now. creative range I can make him very refined or yulo which is going crazy how about yulo >> be more let's let's be more divergent at this point so yeah for the ask redfin section >> based on the business and user go >> and uh user goes that I provided earlier generate generate more design options aspects to vary you can you know let's say layout color schemes yeah why not images doesn't I don't need it text font I don't need it text content yes so generate variations it can probably go pretty crazy because we are in this euro mode right now but we'll see yolo means you only live once if somebody's wondering it's funny that they've put that directly into their product. >> I know. It's funny that you can I have to Google it to look it up every time. >> Yeah, it's from a Drake song in like 2014, I want to say. So, shout out to the rap Canadian rapper Drake making it into a Google product. >> Great. Should have a, you know, gift or image in some sort of success message. >> Oh, man. I think like the song he personally published on this was pretty explicit and so Google probably wouldn't allow it. [laughter] >> Oh, cool, cool, cool. That that makes sense. Otherwise, I was thinking introducing the MV here while we're waiting. All right. Wow. >> Way more divergent. I like this. So, you So, the hack here is use YOLO mode in stitch to get divergent solutions and then we're going to take these from stitch to studio. >> Yes. Um, as I mentioned, you can keep ideiating from here, right? So if I were to go back to um to my if you remember my mind mind map we can just keep diverging from here and you can you can ask a prompt here as well right and there are different models that you can play with by the way for a fast model that you can export to Figma in the future and it's something odd right it should allow you to export to Figma and Google AI studio for any of the models here however for some reason it only allows you to export Figma if you're under the fast mode >> just Just a heads up. Another thing Akash it's redesign model is very interesting. It also give you a lot of creative ideas by doing redesign. So you wanted to check that out. >> Let's see it >> more ideas for the sake of time. Let me just give you a Oh, I think I Yeah, I think I selected the right one. >> You need to make sure to select the right frame so it knows what to diverge off of. >> Exactly. Yeah. And this is a perfect time to introduce any kind of ads or envys. Mhm. >> So back to what I'm saying right here, it really depends our needs. Let's say if we are happy with the idea, we think there are some potential, we can export to Google AI studio for further. Wow. No, it's getting it's getting >> now it's going wild on the design system, but at least it's something different. >> Yeah, that's true. So let's let's say if we still like this one. I know it's getting it's not really what we asked for to be honest, uh Akash, but let's go with this one. So let's say we wanted to uh export to Googlei studio. By default, this is the one. And um I can say turn this into an interactive prototype. By the way, we don't have to provide anything here. We can leave this blank because I'll show you in a bit that it just essentially copy paste the same thing in Google studio. That's that. >> For some developers, I know they like jewels, which is more, you know, if you have a GitHub repo, um you can download code and copy the code as well. But those those are the features that I don't use much. >> So that's like Google's coding agent, right? >> Yeah. That's that's the nature of Google products. In other words, like stitch, Google I studio, they have so many different experiments or products that they are not very well connected in my opinion and that's why we still get to see this stitch to Google studio workflow which could have been more streamlined. >> Yeah. And we previously had director of product Jacqueline Consulman on and she had like Opal and there's so many different products that Google is kind of like experimenting with. Some risk I guess you have is that they just stop supporting it like Stitch for instance. But while they have it, it's pretty useful I think. >> True. Yeah, that's a great point. There's a risk on the other side of being free. [laughter] >> Oh, Stitch is free. Wow. >> Yeah, Stitch is free. >> Cool. >> All right. So what it just did is that it's selecting design and then with the HTML file for reference which is helpful and then with a prompt. I think one thing I didn't do here Akash we didn't have the time to do here is sometimes I I don't just stay with this page. I ask for what will be the page if you if people click on the CTA button >> so you get a full prototype. >> Exactly. >> The screen. >> Exactly. And I I can select multiple of them and export to Google AI studio. >> Y >> So for example, I can select multiple of them as opposed to just selecting one. But for the sake of demo, let's keep it simple for now. >> And then Google a studio is going to go out there just kind of like we saw Claude and Lovable do. And if you had to rank kind of the three like in terms of prototyping, how would you rank the order of them? Lovable at the top. >> If price is not a factor, because I know price is important for people to access to certain tools. If we don't consider price as a bottleneck, then I like Larable. >> Then Google AI Studio then Claude. Is that how you'd rank it? >> Well, Claude, I don't see it as the same tier with Larble or Google Studio because Cloud is really about how to say like a well-rounded nice tool that can do a lot of things. So, I don't put them into the same bucket as Larable and Google AI Studio. So if I were to to rank things, I would more around ranking things within Lavo or Vzero or Google AI studio if that makes sense to you. >> And what would be the ranking of those three? >> I I like lovable the most in terms of how wellrounded it is and followed by Vzero followed by Google AI Studio and those are only judged by design quality like like visual visual quality. >> Yeah, me too. So are there any advanced tips before we close out on Google AI Studio for people who want to become experts at using Google AI Studio? >> I think the something that I if I only to have to say one tip I would say you have the ability to type in the system instruction on the main page. So if you click here it's sort of hidden that you can type sub system instruction here. So for example, you could be very specific about the style that you wanted to generate or you can be specific about other things but this can provide some actual context similar with loverable it has the knowledge base which is a similar idea I would say. So that's Google AI studio. And another thing I wanted to call out is that if you go to the interface, there is one thing which is annotate app. It's similar with visual edit is the counterpart of visual edit in Google studio which is quite interesting. So if you go to add comment, you can drag things and make comments and it's just like a built-in tool for you to add comments. Whatever those comments are, you can act to chat all together to make revisions. >> Nice. So you can kind of more simulate the multiplayer way you would normally iterate on a design in Figma here with AI, >> right? And by the way, I have a I have this tool which allow me to annotate anyway before this feature was introduced. But again, it's an interesting feature in my opinion which is different from many other tools. Yeah, we're talking about these visual editors and I think one other sort of divergent tool we need to cover for people is cursor because they just released their visual editor. So, how would you say that stacks up against these tools we've seen today? >> Yes, I've seen some exciting updates about cursor over the past two months. You know, the browser tool which is exciting and it's launching new feature that is tailored more around visual visual editing which is also exciting. But you know I did a quick table just to explain my point. This may change depending on how fast tooth move. But this is the current state. Like in my opinion I felt like cursor still has a slow learning curve. Even with all those little updates it is not very friendly for non-technical folks or people with minimum technical experience to work to work in still. Um, so that's cursor. Regarding the quality, it can really goes from okay to great depending on your skill sets or like how specific you how specific you do things with with cursor. And in terms of speed, it's just because of cloud artifact which is you know artifact in cloud.AI make things really easy for people and I consider is much faster than cursor. Same with you know stitch and vzero. I consider them faster than cursor just because of fact that they work in a browser based environment. You don't have to set up different things. Um it's it's faster that way. U in terms of use cases I found it helpful if you are really wanted to seriously build something then cursor it's a well-rounded flexible tool compared to those browserbased tools like stitch vzero cloud artifact. So it's really pros and cons here right some people want to have this kind of freedom to do things that they want but a lot of people out there it's just not you know still not comfortable with using cursor and it is okay and those tools are you know better for those needs. >> Okay amazing. Wow this was a master class. If I had to sum it up for everybody who listened, we covered the four key areas of what it means to design with AI. And we deep dove into two really important workflows. Workflow number one, build a custom GPT, a Cloud Project, a Cloud Skill, or Gemini Gem to help you prompt your prototyping tool well because that prompt sets the context. Workflow number two, use a tool like Google Stitch to dream up divergent solutions. So what we saw today was when you use the YOLO mode, the redesign mode features that they've built in, you can generate 15, 16 different ideas. And that's how you get alpha from using AI. You don't want to use AI to just do a worse job of what you would have done before. You want it to enable you with new superpowers because the designers and PMs who have new superpowers from AI, they will replace the designers and PMs who are just using AI generically. It's not like design and PM are suddenly going to get replaced overnight. it's that those people using AI like some of the workflows we demoed for you today will replace others. So if you want to use Genron's custom GPT the link is in the description below. It's also in my newsletter issue where you can find all of the details of the workflows and the mind map that he shared today. Ginron, thanks for this master class. >> Thank you. Thank you for having me. >> I hope you enjoyed that episode. If you could take a moment to double check that you have followed on Apple and Spotify podcasts, subscribed on YouTube, left a rating or review on Apple or Spotify, and commented on YouTube, all these things will help the algorithm distribute the show to more and more people. As we distribute the show to more people, we can grow the show, improve the quality of the content and the production to get you better insights to stay ahead in your career. Finally, do check out my bundle at bundle.ashg.com kashg.com to get access to nine AI products for an entire year for free. This includes Dovetail, Mobin, Linear, Reforge, Build, Descript, and many other amazing tools that will help you as an AI product manager or builder succeed. I'll see you in the next episode.