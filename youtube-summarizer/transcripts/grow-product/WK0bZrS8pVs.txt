What is the most powerful AI workflow you've seen? >> I think the most powerful thing is [music] managing my product process in cloud code and cursor using a bunch of MCPS. >> What does that mean in plain English? >> So basically, you can service all of your product context into your agent or tool of choice. >> What can you do with this? >> Oh man, I think you can do so many things. You can analyze charts. You can automate your weekly reporting. You can synthesize [music] a huge amount of qualitative feedback. You can turn all of those insights into a bunch of specs. And then you can even push them to quad code or cursor to start [music] prototyping directly in code. So, I've got one of the heads of AIPM at Amplitude, the principal PM in charge of their agents and MCP [music] products here today, and we're going to walk you through everything you need to know to master [music] analytics and cloud code. >> I automate a lot of my weekly business reporting using a bunch of dashboard agents. I can easily analyze, run successive [music] queries, pull in a bunch of data and just run a bunch of analysis on my product data. Wow. They made this conscious decision on like how do we make the most powerful agent or what are the most powerful features [music] and set of context they can provide the model so that people can code in ways that they've never been able to do before. What is the biggest mistake people make when using MCP? Before we go any further, do me a favor and check that you are subscribed on YouTube and following on Apple and Spotify podcasts. And if you want to get access to amazing AI tools, check out my bundle where if you become an annual subscriber to my newsletter, you get a full year free of the paid plans of Mobin, Arise, Relay, Dovetail, Linear, Magic Patterns, Deep Sky, Reforge, Build, Descript, and Speechify. So, be sure to check that out at bundle.ac.com. And now into today's episode. >> Frank, welcome to the podcast. >> I'm stoked to be here. Thanks for having me. >> My pleasure. What is the most powerful AIPM workflow you have seen people use these days? >> It's got to be managing your whole product process out of cloud code cursor and hooking it up to some product analytics providers like Amplitude's MCP. >> What does connecting cloud code via MCP really mean in plain English? >> Okay, so there's two different tools. Cloud code, it's a terminal based coding agent with access to a bunch of different cloud models. Um, but it has some pretty cool stuff like MCPS, skills, a bunch of other actions. MCP, it stands for model context protocol. And the simplest way to put it, it's the easiest way to connect your AI models with any external tools, action, and data. So we've used MCP a lot at Amplitude to connect a bunch of tools and data that we have to a bunch of external AI clients. So big ones include claude, cursor, chatbt, and we use that to power a bunch of pretty complex coding and product workflows. >> So what does this setup allow PMs to do? >> So this setup lets you do a lot of things. So I automate a lot of my weekly business reporting using a a bunch of dashboard agents. I can easily analyze uh run successive queries, pull in a bunch of data and just run a bunch of analysis on my product data. I can deeply investigate different trends that are happening in charts. So while I used to be creating a bunch of manual charts, the model is doing all that for me now. It's really easy for me to explain why my metrics are moving because I can pull in all of the underlying feedback data to try to hypothesize about what's what why things are changing. I can drill into specific account health of our users and also some timelines about what people are doing uh within our product just to see like what their behaviors are like. I've been actually using these models to draft a bunch of PRDs based off of this analyze product context that MCP makes so accessible in cloud code and then if you're really venturous you can actually take some of those insights that you're generating convert them into PRDs. If you want to route them to a human, you can push them into Jira or linear via MCP. But if you're really daring, you can also push them directly into cloud code cursor and try to drop some prototypes on your own. >> Okay, so it's really the gateway drug to all of the 10x AIPM workflows that people talk about these days. Before we really get into this, can you show us how to set all this up? >> All right, sounds good. Let me walk you through how this is set up. So I'm actually going to jump the cursor. If you see like this repo, I've spun up my own Amplitude product repo. So on the left hand side I have a bunch of contexts that I've already actually aggregated about all the different product lines that I work on. So for example we've been working on agents AI feedback aski like a lot of our enterprise control features MCP. So I have um specific folders and contexts and files detailing about some of the initiatives that we might have like within our like our road map and some of the specs that I might be thinking about and also just a bunch of these different learnings and like verbiage that refer to like the specific parts of this product. So within cloud code or within cursor, I can easily refer to some of those pieces of context, brainstorm about it, draft new spec. But I think the most interesting thing from the most recent changes in cloud code in uh cloud iterating on this like new protocol called skills. Um so I'll just jump into a specific example, but I've created this specific skill. And the way that I simply think about this is it's a clear like name about what this skill is. There's a little description about like what the skill is and like when to perform it. So that's some metadata that the models can use to determine when to use that function. But at the simplest nature, this is just a prompt. There's a prompt about what is this specific task that I'm accomplishing. What is the set of heruristics or steps that I might take? Uh and then also you can configure what set of tools you might use that is connected to MCP in order to accomplish that objective. And so like we we actually had this model like this tool release recently called like auto insights which I can point it to a specific chart. We'll pull like anywhere from like 20 to 30 of charts that are similar to that based on like relevant properties and then we'll do a bunch of analysis try to find the specific segments or the specific like group eyes and then try to figure out why why things are changing hypothesize about like what the biggest drivers are and for a human that would have taken hours for me to spin up all these charts. We've actually replicated a lot of that using this cloud scale. So I I taught like this model basically, hey look at this chart, pull this data from using Mod's MCP on the URL. You can basically here's some heruristics about like how to identify and like look at specific patterns. So look for spikes, look for seasonality, looks for look for anomalies and then basically taking some of that context trying to look at other tools like within our feedback within our releases within our annotations on like some amplitude data. Can I explain that analysis? And then I give it a format like tell me what happened, when this occurred, what is your primary hypothesis about what changed, some plausible explanations, uh your evidence, and then what this means for us as a business. And so I'm going to jump into cloud code and terminal real quick where I can kind of show you uh an example of like where where this ran. >> All right. So I have sort of two separate follow-up questions here. Y let's first start with the context. Yeah. >> So what is through trial and error what have you learned is the right context you should be including here for cloud code because I know there's always this concern about just using up your context window also. So there is a concept of too much context but what is the right amount of context and how do you organize that so that cloud code can ingest it well. >> Yeah I actually don't worry about it too much because you basically can refresh a lot of the context every time you start a different session. Right. Um so within cloud code you can like within terminal if I feel like I'm running out of context I can just exit out and then basically it'll refresh the context. >> Similarly with uh in cursor if I wanted to spin up a new agent I could just jump to the top like right spin up a new tab and it basically refreshes like it forgets what was happening in like a prior thread and then I have a fresh set of contexts in order to do that. >> That being said there is a bit of context management that you do have to think about. One of the trade-offs of MCP is that a lot of the tools that you might have loaded, so let me show you example. I have Amplitudes MCP hooked up. So there's a bunch of tools that are here. Uh same with linear I have that hooked up. Um so the the context from what tools are available, the names, the descriptions, there's also some instructions tied to each MCP on like what's the specific format that um like this MCP uses or how do we want the data to be used or like some preferences on when or how the tool should get used. So that does fill up some of the context depending on what MCPS you have loaded. That being said, most folks don't have a huge amount of MCPS loaded up. I think even with Amplitude and linear loaded up, I might be using up anywhere from 5 to 10% of my context. So I haven't actually run into that many limitations when doing some of like this product specific analysis. >> That being said, if you are doing a really long analysis, so for example, pulling in a huge amount of chart data, doing a bunch of successive queries, pulling in a huge reams of feedback, sometimes you might run into context issues. Um the one benefit on cloud code is when you start running out of context they have this feature called compaction where it starts trying to summarize some of what happened in that thread consolidates that into some smaller summary and then let you move on given the fact that you had like reached the previous set of contexts and then there's also just like a pretty big like bitter lesson take on it. you can worry about like context management now. The reality is in like the next few months a lot of the new models release they'll probably expand their context windows or have some different mechanisms within agent client to to handle some of that complexity for you. So just build for the future and like think about pulling in like what what's most relevant to give your agent like the best types of responses. >> Yeah, you can use the slash compact command. They actually show you how much percentage of your context window you're using up in cloud code now. So, I recommend for people when you're hitting that 90% or something, don't just rely on the slash compact. Actually, I ran into this problem at about 1:00 a.m. last night where it said conversation too long can't compact. And so, my newest sort of hack is when I'm at like 90% or so because I don't want it to just fail at compacting. I will say write a markdown file of your process and your progress and what you have left to do. That's a good way also to kind of transfer the context over if you need to. So you answered that we can put almost as much context as we want. So what context like can you walk us through specifically like what sort of methods are you using to get context like meeting notes and stuff in there and how have you structured those folders and files in here? >> Yeah, sounds good. At the simplest like I have a product folder. >> Today's episode is brought to you by Amplitude. Replays of mobile user engagement are critical to building better products and experiences. But many session replay tools don't capture the full picture. Some tools take screenshots every second, leading to choppy replays and high storage costs from enormous capture sizes. Others use wireframes, but key moments go missing, creating gaps in your understanding. Neither approach gives you a truly mobile experience. Amplitude does things differently. Their mobile replays capture the full experience. Every tap, every scroll, and every gesture with no lag and no performance hit, it's the most accurate way to understand mobile behavior. See the full story with Amplitude. Today's podcast is brought to you by Pendo, the leading software experience management platform. McKenzie found that 78% of companies are using Genai, but just as many have reported no bottom line improvements. So, how do you know if your AI agents are actually working? Are they giving users the wrong answers? Creating more work instead of less, improving retention, or hurting it. When your software data and AI data are disconnected, you can't answer these questions. But when you bring all your usage data together in one place, you can see what users do before, during, and after they use AI, showing you when agents work, how they help you grow, and when to prioritize on your road map. Pendo Agent Analytics is the only solution built to do this for product teams. Start measuring your AI's performance with agent analytics at pendo.io/acos. That's pendo.io/ a kh. >> Um, so these are some different product lines I'm working on. So agents AI feedback aski enterprise features MCP. So within each of them I might have some context about what is our like Q1 plans. I won't show it because there's a bunch of stuff there but plans like we're thinking about forms of memory like ambient agents different types of agents that we're thinking about. So these are basically specs that I've converted into markdown files that I can refer to pretty easily using like the at command. Uh if I wanted to brainstorm from a given like prd or just say hey we already have a bunch of planning within like our Q1 plan. Can you actually just fetch all of the specific items and then turn those into drafts of PRDs? And so as example I have a few templates actually for some like pretty recurring types of tasks I might do. So like for example draft short PRD. So give me like a sense of the problem context. Talk through like in narrative form what a solution to workflow might look like. Here's how I want you to format or break down that response. Here's some heristics on like how do you think and strategize about what to build. Uh so there's some of like some light heristics that I typically might have on selecting features, emphasizing simplicity like it's okay to say no. Here's how to think about some of the design like principles for some of these features. Uh and then taking some of those requirements that might be in the doc or might even live elsewhere and turn that into acceptance criteria here. Um, so that's just a sample, but if for example in cursor is like like draft like draft a quick PRD using the PRD template. Um, so that it has it pulls this into context immediately has it knows what like what the format I'm looking at and then I could talk about hey talk about this spec specific topic or refer to this doc where we have a bunch of ideas and like structure that into a better format. So that's that's one example. I guess the other thing you touched a little bit on granola. I've been playing around with this. I use cloud code actually to try to create some of these light little automations or connections that I previous previously wouldn't have. So my understanding as of now is granola actually does not have a dedicated MCP right now. So I I try to hack cloud code to to build some type of automation so I can adjust some of my calls. So there's if you see here there's some like notes say like the year month uh and then there's there's some like meetings that I've I've had and so it tries to inject some of like the the summarized notes there. So this for example this is like our MCP standup that happened. I basically could run a command. Can you pull in my recent um granola notes using the script we wrote? Um let's hope this works. I I haven't run this in the last two days, but basically like that's that's also a part of my product repo where we I use cloud code to create the script and it's going to run like this command terminal and then yeah, you see there's a bunch of these meeting notes that like were previously ingested that auto get dumped in. Um so if you want you can refer to like the whole folder. So, I'm like, hey, at 2026, um, or that's like a specific one, but like the at 20261 folder here, you immediately pull all of those notes in context. If I wanted to say, summarize the main takeaways, give me the specific action items, I can put that into another folder. But basically, all this stuff is immediately accessible if you needed to cuz it's all like consolidated in that one system. >> Amazing. I love this use case. I think most people should be using this specific one. So, how do you get around the MCP? He created this little function or skill or script in this case that he referred to it with cloud code so that he could access those granola meeting notes. And how are you using obsidian and conductor in your workflow? >> To be quite honest, not too much. Um I think those are the power tools if there's like specific nuances on for example conductor, right? So I I have it pulled up. Conductor is like a great tool when if you have a bunch of coding agents that you want to run in parallel. The problem is often times if you have one branch of your GitHub and you're trying to push changes, if you kick off a bunch of like cloud codes in like the same branch, often times they might have a bunch of changes that conflict with each other. They don't really they're not really talking to each other. Um, so the thing that they nailed on conductor was they made use of this feature called a git workree which essentially copies your repo into a separate like work tree. So then you can run multiple cloud codes in parallel branches. they won't conflict and basically you can decide to change like to merge some of those changes like together if you're like going into the huge amount of like agent orchestration workflows if you're running you're trying to run five to 10 of these coding agents in parallel conductor is probably the best-in-class tool that I've seen for that I know there's a lot of excitement in the industry on what is this orchestration like framework looks like but this is the prettiest and best that I've seen so far but for just to call out for specifically product management I don't really need too many tools like it's not a big deal for like this model to conflict like if I'm kicking off a task like usually I'm I wanted to create one PR here for this team or I want you to do an analysis here. So there's not as much of an issue where like the code conflicts within this branch and so it's it's honestly probably easier to just do it directly within different tabs in terminal instead of having to spin up a bunch of git work trees. But if you're doing specifically these coding tasks trying to do it all within the same branch is basically just going to lead a huge amount of messiness. >> And where does Obsidian come into the workflow? >> Yeah, so it's it comes with personal preference. Obsidian basically has access to your file system the same way that a cursor also does. If you hate staring at markdown, then Obsidian is like a pretty wrapper to to directly make edits. Like it's more akin to like a typical text editor. But me personally, I love playing around with Markdown specifically. I like spend a lot of time coding myself and like it reads pretty well to me. So it's kind of just nicer to to have the same tool where I can push code, push changes directly and use the agents natively using cursor or within cloud code. >> Yeah. And you can also just doubleclick or right click on any of these MD files and hit open preview or I do up apple shiftv and you can look at these pretty. So I feel like the obsidian hack it's not really necessary. So yeah we got people to set it up. What are the top use cases of cloud code and MCP with analytics for product managers and product analysts? >> I think there's five things that I'm doing really heavily. So first one, anytime there's an anomaly or some type of big change in my metrics, I'm using some skills and tools within cloud code in our MCP to analyze and that do some deep chart analysis. So I drop in a link and it basically the agent can figure out exactly what specific segment or what little group by or what specific related metric might hypothesize and lead to that change. So that saves me a bunch of time. I think the second thing is I've automated basically all of my like weekly business reviews and dashboard reporting at Amazon. I used to spend all of my Sundays just coming up with with our metrics and explaining why things are happening. Now I've pointed this to cloud code and some of these like amplitude dashboard agents and Monday morning I come all the five to six dashboards I look at are automatically synthesized. I know exactly the three to five top insights for me and I know the one specific urgent issue um to tackle with my team. So don't need to analyze like the dashboards anymore. I can just start like focusing on solutions with the team. And the third one is doing a bunch of deep qualitative customer feedback analysis with AMP's MCP and the AI feedback product. All of our tickets from Zenes, Slack, Amplitude Surveys, that's all unified in one place now. So I can just point cloud code, tell it to analyze the feedback related to AI, agents, MCP, and then the agent will navigate all of our tools, consolidate all the data, do its analysis and clustering, and then give me a clean report of exactly what the urgent issues are, what the bugs are, uh, and then what's the single most thing that people love about our product that week. Fourth one, spend a lot of time taking those insights coming in from this dashboard agents or like this cloud code chart analysis and then given this context, can you think about what's the right set of actions or plan to do to resolve this space? Um, so I've given this agent some heristics on how I think about coming up with solutions and the agents the agent can go at length explore a bunch of different ideas. I can drop in some images and then it can just turn that into a spec so that I don't have to write it anymore. So, Opus is like the best thought partner that I've ever had on coming up with some of these like design and product ideas. And then lastly, I've kind of totally reshaped like how I think about what I wrote to the team to build versus what I just do myself. Now, I can take these PRDs or take some of these problem statements that the agent has surfaced. If it's easy, I just drop it into cloud code or or cursor directly and then it could pre-draft some of the solutions for me while I'm in a meeting. Um, but if it's a really tough problem, I can go ahead and hit the linear MCP and say, "Hey, I want to route this to the AI capabilities team or like the MCP team, file it under like this specific project and like assign it to this engineer and then assign it to cursor as like a courtesy to hopefully like soften the the workflow on their end." So, these are the five things I'm doing the most frequently. >> I love these. This is the new Vibe PMing workflow. These five use cases. Can you demo each of these for us? So just for context, this is one chart that I'm looking at on the MCP side. We're curious about like what is the average number of tool calls that might happen uh in a given week. And I'm seeing this anomaly where like the average number of tools being hit is 8.1 for this week, which is quite different. It's something like that's worth investigating because I need to figure out how do we manage the right tools, the descriptions, and make sure nothing's broken. Right? So I can take this link. If I jump into cloud code, like I said, I had a bunch of these different skills that I had. So there was the analyze chart skill which I'll just preview really quick for you here. Analyze chart. So remember there's a name description and a bunch of heruristics on what to do about it. If I do the slash command basically that can trigger that specific skill for you. So analyze this chart and explain why uh there's a jump this week. So just jumping in here. So this might take a bunch of time but now the agent is connected to the amplitude MCP. Um so you can see here it's parsing the URL. is starting to pull in some data on the chart and then we have some tools that let you navigate the data. Um so there's a chart data we can query the underlying data set and then basically the model starts showing its work on how it's navigating your taxonomy, how it's navigating relevant charts, maybe it's navigating the specific events and properties related to that. So this might take a few minutes and but basically you it's you can see the agent reason about which charts it's looking at, which events it's looking at or what are some hypotheses about like where this might change. is it's navigating the properties. It's breaking it down by like what it thinks might be that spike. Now it's pulled in a different chart. And if you think about the human equivalent of this, it's the same as me having five different versions of this chart. I had to go like manually group and cut this um by a specific property that I have in my head or I might have a create a separate chart uh where I I look at a different event that's like somewhat related to it or like there's a bunch of this manually manual like iteration on the guey that I had to do like creating a bunch of different these charts and now the agent is doing all of this just via via terminal. So you can see here this is the format that I specified for it but it says here's the chart here's the time range I looked at um here's the metrics that analyze and it walks you through like that a spike happened when it happened the underlying data that led to that and then some hypotheses about it so detected that there's a change within like the types of tools that we had analyzed uh enabled it has access to our experiments and feature flags data so it saw that these there is some changes within what feature flags we had enabled uh which provides different tool access and then so you see some flag activity there's some differences in historical precedence and then hypothesized about something else that might have changed. Maybe it's data quality, a different power user. So basically like what I would have had to do manually to investigate explain what's happening. The agent just I think it literally did this in like a minute and a half and it this would have taken me probably like one to like one to three hours depending on how complex the query was. >> Yeah. And that's with having a background as a data analyst. If you're just not having that background, it might take you half a day. >> Yeah. And and that's assuming you know what which properties to look at like basically because at least on the the AMP MCB side we've done a huge amount of like configurations on how do you simplify the names of the tools and the descriptors or how do we improve our underlying search infrastructure or how do we capture the right context about when to use a specific chart or when to use a specific dashboard or when to use a specific event and that context is all provided um to the agent like via these MCP tool halls so that it can more accurately like navigate that data for you. And so if you compare this to an expert of the taxonomy, it's probably the difference of this running in 1 and a half minutes versus them spending 10 minutes navigating the chart. If this was a person that did not know that taxonomy, they would have either gone the wrong answer or they would have had to spend like two days chasing down the right person in the or that could tell them what to look at. This agent basically was able to figure it out itself given a lot of this like underlying like semantic infra. >> Very powerful. So deep chart analysis. I think I understand this use case. Really the powerful thing here is you write yourself a skill and you teaching it how to navigate the MCP and make the right tool calls. >> Yep. Exactly. >> Totally understand this workflow. What's the next workflow? Well, that was around automating dashboards, right? >> Yeah, the next one's around automating dashboards. So, similarly, I haven't analyzed dashboard scale. So, here's when I use it. meeting prep dashboard investigations connecting quant and I can show like I instructed the agent to look for the URL query the underlying charts fetching them like at three at a time because there's some context limitations like when you're interacting with some of these tools look for these specific things like if it's a KPI chart like look at this and the percentage change if it's a bar chart look at concentrations or gaps so teaching them some heruristics on like how I as a human would have thought about like this data and then what's cool is like now that the this agent has analyzed a bunch of charts and figured out like what's happening in the dashboard. I I pointed it to hey go go go pull the feedback data recently and see if there's anything relevant there or we don't have it here but like we also have access to like session replays or experiments. So basically depending on what data you have and like whether it's relevant to a specific dashboard you can instruct it to also query that as part of its agentic workflow. Uh so this is uh and then this here's like the takeaway of like how I wanted it presented just personal preference on what I I think is helpful some best practices but in reality like let me let me show you. So I I'll I'll start over just given the the context piece, but here's Claude analyze dashboards. Let's pull in. So we're talking about MCP. So can you give me a TLDDR on my external MCP usage dashboard? And so this is different than last time because like last time I I gave it an explicit URL which is and this time I'm kind of by giving it some some like some natural language query, right? So if you are an MCBPN, you have to spend a lot of time thinking about, hey, a lot of folks are going to give you pretty ambiguous natural language questions. How do you like take those queries and then like teach the model to figure out how to use your search infrastructure or navigate your data so that it pulls the right thing? So I have a dashboard that's called external MCP usage dashboard. So it's it's quite it's like kind of cheating. Um, but if I told it said, "Hey, pull the MCV dashboard." You can teach it on the tools to be like, "Hey, here's how you think about using like the search endpoint or here's how to filter the data properly or run a few different like searches and then figure out like reason about what's the best response." So, there's a lot of different little experiments you can do on like optimizing the tool. Um, but now you can see it started looking for external MCB usage, MCV usage, like these are different search queries it was running on our search endpoint on the dashboard data. It pulled it, it identified the dashboard and then it has a different tool for querying those charts. So, it's pulling in those three diff like those different chart ids three at a time because that was the instruction I gave it. Uh, and then now once it's pulled all of that data in context, it gives me a quick overview about what some of the metrics look like. So, won't spend too much time on the metrics because that's like that's our metrics. But you kind of see like, oh, once again, I could have spent the 10 minutes scanning through like all of the charts in my dashboard. And then if I found anything that was interesting, I would have to like copy paste some of those individual data points, pull them into a specific like part of like the bullet points that I would summarize. I'd have to think about the narrative. Um, I'd have to jump into my AI feedback and run a search on MCP and see if there's anything relevant. But the model given those instructions did it all in what was that like a minute and a half as well. Like >> that that saved my whole Sunday again. [laughter] >> So that was the promise right? We were going to save our whole Sunday. So if I recall like I used to do this too like when I was at Epic Games PMs used to have a pretty pretty much do the analyst hat as well. So I remember on Sundays for my Monday meeting I'd spend like six hours going to various Tableau dashboards having them load copy pasting them interpreting them analyzing them. How do you put together that whole workflow of like these are my 25 typical WBR slides, go pull those all for me? >> So, we don't currently do WBSR anymore. Like I have agents that are automatically pushing these reports into every single product team's channel. So, like instead of having to like sit now at Monday and like review the metrics, all of the main takeaways, all of like the goals, all of the main deviations, there's already a draft of that there. And then basically like each PM like myself included, we can have our own planning meetings with like our specific whether that's our stakeholders or within just my engineering team. Uh if if we want to talk about the metrics, we can uh or we just spend the time more focused on like the tactical as opposed to like the reporting. >> How do you schedule things here? >> I do this um via the dashboard agent that's in Amplitude. I don't if you want me to show that real quick. >> Yeah, let's see it. >> Yeah. So this is like the we've had this product for a few months now, but this is the Amplitude agents um like kind of inbox. And so you can see here I have a bunch of these different scheduled these different scheduled agents went to the different dashboards across the company. So automated insights the overall like AI native amplitude dashboard like analytics foundations like website optimization like there's a bunch of these specific agents that I've spun up based off of dashboards I've created. Um so there's a dashboard agent I pointed it here. I gave some context about like you can give it some context optionally like we have a pretty robust system prompt on how to think about this but if you want to do weekly monthly business reviews you can if you just want to report on a new feature there's a bunch of these little prompts that like basically we want to guide people on how to do it and surprisingly to me like I came from like a startup background most recently when you're dealing with enterprises like most folks don't like they're not super familiar with Asians they're they're not sure like what knobs they can turn when they're creating an agent so giving them really easy understanding of how to create it what system prompts what the Asian is steered by is extremely important. We had a pretty low agent creation flow until we brought in these starter prompts and then we saw the relevance and like the agent creation conversion pretty dramatically improved once we did that. So this is just an example of like this within the the agents page. These get pushed all like into Slack into email. There's a bunch of folks that are consuming it there. People actually don't spend that much time on these asynchronous agents within the product. Like the idea for at least these specialized agents is let's get them into all the places where people do their work. So on Slack, if we wanted to push that into email, if we wanted to push that into Teams, or if we wanted to push these insights to the GitHub, like there's a lot of work that we do on the agents at MCP front, like whether that's via partnerships, integrations, etc. to get these insights to like where they're most helpful at. >> Makes sense. So I think I understand the second workflow. What's the third one? >> The third thing that I do religiously on those Mondays, so we've pulled in feedback from a bunch of different places. So this is the new AI favorite product launch product that we launched in November. We're pulling data from Zenes, Intercom, Salesforce, Gong. There's a bunch of stuff coming in from public channels, G2, App Stores, etc. And then like it's not here, but we also have an integration with Slack that we're we're prototyping right now. So all that data gets piped into AI feedback. It gets auto analyzed, but sometimes like it's not exactly what I want. Like this is a specific tag or like insight that I have, but I might have like a specific point of question I might have for it. So let's start a new instance of Claude. So once again I have a specific skill that I've created in the template for that analyze feedback pull in my agents MCP and other AI related feedback give me TLDDR. So all so but one when I was working on an teams would basically do all of this stuff in like spreadsheets. There'd be huge amount of these like customer surveys or they would be copying pasting messages from Slack or they'd have to send like this crazy integration with Zapier to like point like pull in data from one place into another. Our product pulls all this in one place for you and exposes it via like an MCP tool. Um so we have two different tools we've tuned for this one is for those pre-processed insights that I showed you. We can provide access to that and then we also provide access to the underlying feedback meion data. So you can see here that the MCP is looking at what is the set of insights that are already created, but also what is like some of the underlying data that's most recent and then it's going to use that to generate a quick response. >> Mhm. AI is writing code faster than ever. But can your testing keep up? Test Cube is the Kubernetes native platform that scales testing at the pace of AI accelerated development. One dashboard, all your tools, full oversight. Run functional and load tests in minutes, not hours, across any framework, any environment. No vendor lockin, no bottlenecks, just confidence that your AIdriven releases are tested, reliable, and ready to ship. TestCube scale testing for the AI era. See more at testcube.io/ aos. That's te s kub.io/ a kh. I hope you're enjoying today's episode. Are you interested in becoming an AI product manager making hundreds of thousands of dollars more joining OpenAI anthropic? Then you might want to do a course that I've taken myself, the AIPM certificate ran by OpenAI product leader McDad Jaffer. If you use my code and my link, you get a special discount on this course. It is a course that I highly recommend. We have done a lot of collaborations together on things like AI product strategy. So check out our newsletter articles if you want to see the quality of the type of thinking you'll get. One of my frequent collaborators, Pavle Hearn, is the build labs leader. So you're going to live build an AI product with Pavvel's feedback if you take this EIPM certificate. So be sure to check that out. Be sure to use my code and my link in order to get a special discount. And now back into today's episode. Here's the dirty secret about prototyping. You spend two weeks building a prototype. You validate your assumptions. Engineering loves the direction. Then what happens? You throw the whole thing away. Bolt changes this completely. When you prototype in Bolt, you're not building throwaway mockup. You're building real front-end code that integrates with your existing design system. So, when you hand it to engineering, they don't throw it away. They ship on top of what you've built. I use Bolt every single day. I host my land PM job cohort on it. And honestly, I'm up till 2 a.m. some days just vibing in the tool, having fun, and building. That's when you know a product is good. When you're using it past midnight, not because you need to, but because you want to. Check out Bolt at bolt.new/acos. That's bt.nw/ aka link in the show notes. >> Basically, there's a lot of people that have to do this like the aggregation like and analysis manually. So, you can see this is a format that I specifically like, but you can also provide the Asian free form context of I actually don't want to look at like urgent issues and feature requests. I only want to look at praises or I only want to look at references for for competitors. So just explain that in natural language and then the agent can like kind of figure out it's like it's right response for that. This is actually like what people are saying about some of like the agent like products. They want more context. There's some like inaccuracies like sometimes there's degradation in quality. Like these are all things that like we hear sporadically here or there in different channels. But having it all in one place, having it in a specific report, if I need to like change it in a specific format, it's so easy via like cloud code or like the next thing I'll demo for you is like, okay, can you convert all of these recommendations into specs and place them in the agents folder? So let's see if this one rips correctly. But >> so this is that fourth workflow. Once we've gotten an insight from our feedback, customer feedback themes, let's convert it into actions or specs. >> Yeah. So this is a live example. I like typically I actually do this in cursor because I want to use my specific draft prdate, but you can also do this in cloud code because in cloud code it's all tied to the same thing, right? I I have this whole product repo in my GitHub. So you can navigate there. You can navigate it on cursor. I typically find that cloud code is a little bit more powerful. It it takes a bit longer, but like the response quality is is marginally better in cursor. But the cursor like there is some benefit of having like a dedicated IDE experience doing the app references like being able to swap between different models and testing around seeing my my specific file system and making edits directly. So I always get hesitant when people are saying it's one or the other like I use both extremely heavily. >> Yeah, I pretty much use cursor because I can use their really fast models if I want to and then I use clawed in cursor which you can also do. >> Yeah. So you can see like I think they're starting to add a few within this folder. But yeah, you basically can create a bunch of specs using one or the other based off the templates that you have. Uh and then like so once again I have a few MTVs connected. There's an amplitude one. There's context 7 which is makes it pretty easy for you to fetch docs from from other repos to explain what's happening. Uh and then I have our our linear connected if I want to push tickets. So so for like some of these longunning tasks that I need to coordinate a lot with the team, I'll push them to linear so that people can track it. We're also kind of in a weird era where there's a lot of tickets I don't even push into linear anymore. Like I might just have take this context and just message the engineer directly in Slack >> just removes one part of the steps or overhead there. So >> and it can even Slack is an MCP. So you can even do that through here. >> Yeah. So this one takes time. There's there's like six different things but basically it's it's drafting these specs. It's putting them into markdown files. I didn't specify the specific PR template, but like uh basically like the first draft of it is being generated based off of this analysis and this context that I pulled in via the the feedback. >> Can we look at one of those and see like realistically do you think those are quality? Would you use those? And right now we're using Sonic, guys. So, it's a little bit faster, but it might be a little bit lower quality, but let's put it to the test. All right. So, it's generating these specs. Can we take a look at some of these and see if they really hit our quality bar? >> Let's do it. And by the way, can you explain that branching GitHub toggle that you're doing in the top left for people who don't know? >> Right here. Yeah. >> Basically, all of these changes that are happening in Cloud Code or in cursor, they're happening in a local branch on GitHub. So, there's a copy of this whole repo that's on my local desktop. >> Mhm. >> When I feel like I'm happy about the set of changes, I can just push it directly into like my the cloud like my GitHub cloud instance of this repo. The same way you'd think about a codebase, like I kind of just have my own product system that's in GitHub so that I can access it wherever. Cuz the other thing that this unlocks is if you're on cloud code on mobile, you can refer to your GitHub repo that's in the cloud and then also come up with ideas and kick off like asynchronous tasks as well. >> Oh, that's super powerful. >> Yeah. Um because you're not always going to have access to cursor on your phone or like cloud code and terminal on your phone. For example, like all the stuff about Cloudbot, like I'd be hesitant to put that onto my work laptop and try to steer cloud code via like a remote branch. But if I have the all of my repo and all my contexts in the cloud, I can just kick off cloud code directly within the cloud app. >> Nice. >> Yeah. >> What's the command for somebody to set this up the first time? Duplicate my repo into GitHub or something. Just simple like that. >> I had just gone into GitHub and created like a new like repository there. >> So when I set this up, I just follow the typical GitHub instructions that they have like in instructions on how to clone a repo to your local branch. Yeah, I think you can actually just do it via like cursor now. Like if you wanted to be like for example, if I was to go on GitHub, if I was to go on GitHub, I have this like AMP product repo. So there's like a bunch of different ways that you can clone a repo to your local. But instead of following those instructions, you can either use cloud code or cursor and just say like let's say this was a fresh repo. I could say, can you copy this to my local branch and sync this to GitHub? And now since it has web and terminal local terminal access, it can manage that whole set of instructions that previously you would you would have to do by hand which is pretty tough typically for like a non-developer. But now like the agent now that it has local like terminal access it can it can do it for you. >> Nice. So how are these PRDs looking? You're the actual PM on this. Is this a good PRD? >> They're these ones are correct. They're too verbose. So I can go back to my prompt and say, "Hey, dramatically cut down the words or the narrative solutions is like way too con like it's way too long. Condense this to like the bottom like one or two paragraphs or hey these these might be too much like too many acceptance criteria. Prioritize to think about like the top three. So if I wanted to like give this there's like command L this is too long. Can you make this way more concise?" So this this is total byping that like how uh how vague those instructions are. But um >> and I think we probably could have made it better by using Opus and pointing it to your PRD template. So this wasn't necessarily the favorite. >> That's the way to do it like for the overall system. But um this is kind of the reasons why I like sometimes doing it in cursor cuz you can also like highlight a specific section and then if you do command K you can give like quick feedback directly there. Or if you wanted to do command L, I can like basically the context in the agent is pointed to those specific lines in that file. Nice. >> So within within terminal it's a little bit harder to specify like what exactly like the change should be but like within directly within the file I can reference it here. >> Very cool. >> This one is like much more concise and more relevant. So yeah it goes back and forth. >> So let's go ahead and say you've iterated the spec is good enough. I think we said the fifth workflow is either pushing these into linear or doing them yourself. So can you kind of show us both of those? I'd be nervous to kick it off, but like one example of something that we have um we have this channel where we are like basically calling out to cursor or cloud code within like an open channel about some instructions that we want to push in our codebase. So >> I might take this prd or like the specific acceptance criteria and I can say like at cursor like slash the JavaScript repo make this change to the AI feedback page or make this like prototype this change there and then you can spin up a background agent there or depending on if you have like a cloud code set up on your terminal like this is like me pointing to our JavaScript repo. So if I wanted to like pull pod I can basically pull a pod and push a change there. Or lastly, like I have the JavaScript repo as well. Like assuming I'm on like the the relevant branch, I would just dump that into here and see like create a branch for that. So I don't want to kick it off like basically I'd want to handhold like the actual code being drafted. So that that that's that would be one route. >> Yep. So basically guys, that's the end toend workflow starts with the deep chart analysis. You got your automated reporting. You've investigated customer feedback themes. You've converted that into an idea and then you take action. You might even code it or a prototype of it yourself. What is the biggest mistake people make when using MCP? >> I think there's two big ones. One is just kind of the wrong expect expectation settings for MCP. Sometimes people think that MCPs can do everything. Just the simplest way to think about it is MCPS are easy ways for your your AI to interact with external systems. They're not complex ways to do workflows. They're not complex tasks already. Like there's a bunch of other tools more relevant for that. But just when setting the expectation of just being able to pull in data is the first step. I think the second problem is sometimes people get excited and they connect a bunch of MCP servers. Some have a lot of tools, some are just totally irrelevant to a given workflow or that in that repo. And so this comes back to the problem with MCP where if you have a huge amount of MCBs connected, too many tools that are not being used, all of those descriptions about what tools are available, what descriptions they are, what formats are needed, they start being provided as context to the model on a bunch of different queries that might not be relevant. And so that that might lead to higher latency on a given response or slightly skewed or like inconsistent responses because the agent is thinking about these tools even though it's not relevant to the task at hand. So just be thoughtful about when you decide to incorporate an MCP tool. For stuff that's not as relevant, you can hide or remove those tools typically within cursor, cloud code, etc. Like you can remove stuff. And then if if you're actively developing yourself, you also need to optimize on what is the right set of tools that you want to make available in your MCP server. what are those things named and then making sure that there there's not ambiguity for the model on which tool to use and there's like accurate descriptors and accurate like instructions on what formatting is required depending on what you're using it for. >> What you just said there these breaking the fourth wall behind the curtain insights of what it's like to be an MCPM have been one of my favorite parts of this conversation. So I think you're uniquely qualified to answer this question which is that people say MCP is an unreliable or a bad standard. What do you say to that? >> All right, I came prepared. It's a slide. Um, so I see a bunch of things related to MCP. One, people just have been overhyped about what MCP can do. The second is MCPs waste a lot of context. The third is there's a lot of painful experiences on how people like set up or have to deal with authentication when interacting with MCPS. Uh, and then lastly, and probably the most important thing is pretty complicated on how to actually configure an MCP. Um, so those are the the four the criticisms I see most frequently. And so I think the main response is set the right expectations for yourself on like what SAP is used for. It is by far the easiest way to connect like external systems with most of the AI clients uh that you're using and instead of like as a PM I had to pull for this huge road map of integrations and spend a lot of engineering resources building them out. Most like AI native tools at this point are exposing a lot of their tools data and actions via MCP. By far the easiest way to get started. Second piece is MCPs required like some amount of optimization like you need to go back and forth on what is the right set of tools you want to make available in your server. You need to optimize the names of them like data sets or actions you make available and then whenever you're finding edge cases it's similar to some of the eval driven development that you see for agents. Once you find find those edge cases, figure out how you can optimize the tool names, the descriptors, and once you come to the right set of tools, those servers and those integrations like work very seamlessly and like kind of power a huge amount of these like complex workflows. And I think the last thing is most of these like AI clients, Claude, Chat GBT, like Herser, everybody's all converged around like the same standard on MCP now. Like it's the standard. Most people have some type of manage connector flow where instead of having to deal with this like manual URL setting that is in like the first versions of these tools, you could just click one button and like those MCPs are set up for you, assuming you're logged into the right tool that you're trying to connect to. Office is handled. So now that it's the native standard and everyone's converging on like the same like standard, it's actually pretty seamless to get started now. And so and then lastly, there's been a lot of this criticism about like context rot when there's a huge amount of tools within the last month and a half. That's actually dramatically changed depending on the AI client. There's like pioneers like cursor, they've done um some experiments around like dynamic tool calling. So instead of having the tools all called every single time, they actually like use rag to find like the right set of tools to call given a natural language question. So you're not using all of your context all the time. Same within Claude like they've adopted some some type of similar standard for not using like all of your context across these tools. And then that's also one of the reasons why they built skills. skills gives you some instruction on here's a specific task, here's the instructions and like the prompts that only gets loaded in context when the model thinks it's relevant to pull that skill. So you can kind of circumvent a lot of these problems with MCP by pairing it with like the right changes on the client side or with a skill to instruct when to use that specific tool. >> Yeah, for my money skills is one of the most important releases. So we started with it, we're ending with it. Make sure you guys are learning those skills. All right, so we've talked a lot about MCP and Claude Code. What else is Amplitude cooking up? What are the most powerful agents you guys are building and releasing? >> So, we're super excited about February 17th cuz we're bringing agents across the whole platform. I wanted to give some context about like Ampitude and its journey. Um, and internally we actually think about this launch as kind of our cursor moment. And the reason why this matters is when we think about cursor, they made coding accessible to so many folks that had never coded before. It was the first foray for a lot of folks on how do you like spin up like your own GitHub PRs or how do you pull in like AI and agents into a coding workflow and it brought so many people and lowered like raised the floor for a bunch of nontechnical folks that were never able to code before. But what's interesting on like the cursor's side is when they think about product development they're always trying to raise the ceiling of what's possible within software engineering. they made this conscious decision on like how do we make the most powerful agent or what are the most powerful features and set a context they can provide the model so that people can code in ways that they've never basically been able to do before. So with our agents launch we've genuinely think this is our cursor moment there are there has historically been problems where folks they're not super familiar with the data tonomy or how to navigate like the amplitude UI or what actions or like ways that they can improve their product are available on the platform. We've built an agent that's embedded across the whole platform. It has access to every single data point, every single action and like improvement that can be taken in Amplitude and we make it possible for people to navigate the whole product just using chat now and using this agent. So really like basically raising the floor what's possible for them. But what's even more powerful is given how powerful these tools are, these actions are, we're actually seeing these like 10x like analysts, 10x marketers, 10x like like product managers, they can do so much more than they used to like given some of those workflows that I showed you in cloud code. And so more people are creating like so many new guides and surveys or like running a bunch of new experiments or like being able to like deeply analyze and like understand trends that are happening across their data all without the same amount of manual time and also like navigating that data in ways that they were never able to do before. Uh and so like these there's these power PMs, these power engineers that have access to way more data that that they've ever had uh previously and can accomplish and orchestrate some of those actions within amplitude but also within like external places like cursor, cloud code, Figma, etc. So just jumping directly into what we're shipping uh there's four parts of this launch. One is this embedded amplitude agent. So we call this the global agent. It's it lives across the platform. You can pull it in a side panel in chat. You can also just navigate on any single page. It has access similarly to cursor of whatever is on the page and all of the underlying tools and data that we have available in the platform. We also have the second part of the launch which we're calling sub agents. So these are more of these specialized asynchronous agents. So there's a dashboard agent that auto monitors your dashboards for anomalies, automates your reporting. We have a session replay agent. Analyzes hundreds to thousands of your session replays. Understands qualitatively what people are doing in your plat like your platform, what bugs people are running into, what issues uh like they're stumbling on, what conversion blockers and telling you exactly like what people are doing. And we built a feedback agent that pulls in all this qualitative data, cuts it in different heruristics and tells you exactly what customers are feeling and saying. And then we have a website optimization agent that scrapes your website and understands ways to optimize your copy. optimize different components on your site and then recommends you a bunch of different experiments you can run. What's underpinning all of this is MCP like we have MCPs provided to these these internal like this amplitude agent these sub aents but we've also made all these tools available externally. So if you want to spin up your own bespoke workflows and agents you have access to all of those creation those read those edit types of tools across all of the data and actions available in the Amplitude platform. And so we've seen these power users build super complex internal workflows. They're mixing and mashing data from like one specific digital analytics platform with through Tableau data and then they're autopiping like these insights and anomalies into like like cursor similarly to how I I showed you and building a lot of these bespoke agents using your MCP. Uh and then lastly, we want these agents everywhere in Slot like everywhere that you work. The first thing that we're releasing like in February, this amplitude embedded agent is available in Slack. You can ping it. Like I think about this as like the big short, you know, this this is my quant. Like anytime I have a question about what data needs to be pulled, what analysis I want to run, I can just hit amplitude agent directly in Slack, it's going to run for a few minutes and it's going to tell me exactly like that analysis without me doing any of the work. I can link back to exactly what charts it looked at and it it can also change anything within the platform. So if I need to edit a feature flag, if I want to run a different experiment, if I want to like watch a survey, like a session replay, you can kick it all off in Slack and then have follow on conversations with it. >> Super powerful. If people want to learn more, contact you. Where can they find you? >> If people want to get started, you should jump to amplitude.com. If you want to see what's happening within this launch, you can go to amplitude.com/ai. And once you're jumping into the product, regardless of what plan you are, you can interact with our agents. You set them up in Slack. You could connect to all of our data within Claude Chatbt, Cursor, Claude Code using MCP. And then if you want to hit me up directly, I'm Frank. Twitter, but you could also hit me on LinkedIn. Frank, thank you so much for this master class in using cloud code, using analytics, and the future of agency analytics. >> Thank you. >> All right, guys. So, we've walked you through what I think is the future of PMing. We were joking off air. This is vibe PMing. Just like there's vibe engineering, vibe coding, vibe designing, all these different trends. This is vibe pming. We walked you through five workflows using cloud code and MCP connected to your analytics tool. And by the way, if you don't have an analytics tool that is pulling in your Gong data and your Salesforce data and the other data, use those MCPS to pull in that data and your Zenes data, etc., and inform your insights, create specs, even potentially code something like he showed you using a cursor or cloud code agent or create the relevant tickets in linear to get your engineering team to do it. This is the future way PMs are going to be working. If you are not in an org that is giving you this access that is allowing you to do this stuff, you should be making the request. And if you're a product leader, you should be using this video as an example to get everything through your IT departments because this makes PMs much more effective. When we talk about all PMing PMs becoming AIPMs, this is exactly why everybody should be getting cloud code access. Everybody should be getting cursor access. Everybody should be getting GitHub access to the codebase. This is what not just teams like Amplitude will be doing, but the Toyotas and the Fords and the United Health Groups and the Mount Sinai Health Systems in two or three years, they will be doing that, too. And there's going to be a huge advantage if you are one of the teams that adopts this sooner. So, if you need more help on this, be sure to check out my newsletter where we're going to have a deep walkthrough of everything he presented today. Check out my other tutorials on MCP which is with the CTO of Zapier with Claude code with the full stack PM on this channel to learn more to make sure you're comfortable with it and send me what you learn what you built. Share the insights so that we [clears throat] can continue to create better content for you and I'll see you in the next episode. I hope you enjoyed that episode. If you could take a moment to double check that you have followed on Apple and Spotify podcasts, subscribed on YouTube, left a rating or review on Apple or Spotify, and commented on YouTube, all these things will help the algorithm distribute the show to more and more people. As we distribute the show to more people, we can grow the show, improve the quality of the content and the production to get you better insights to stay ahead in your career. Finally, do check out my bundle at bundle.ashg.com akashg.com to get access to nine AI products for an entire year for free. This includes Dovetail, Mobin, Linear, Reforge, Build, Descript, and many other amazing tools that will help you as an AI product manager or builder succeed. I'll see you in the next episode.