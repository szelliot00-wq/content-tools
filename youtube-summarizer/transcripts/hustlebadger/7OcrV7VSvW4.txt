Okay, super. Well, I'm going to start kicking off. Um, just to introduce myself, I'm Ed. I'm one of the co-founders of Hustlebadger. Also got Susanna, who's my co-founder on the call and um, she'll be active in the chat, probably dropping in a few links in there from time to time. Um, great to have you all here to talk about evals. Um, if you do have questions, please do stick them in the chat and I'll do my best to uh, get around to them. um either as we go through or uh at the end. Um just in terms of housekeeping, yes, this call is being recorded. Yes, it will be on our YouTube um in a couple of days. Um and we will drop you a uh a little follow-up email with some of the links that we share uh as well. Um please do stay on mute if um if you're not asking a question uh at the end and it just helps everyone to uh hear a little bit better. So with that uh I bring you AI evals. Um let's talk about evals. There's there's a lot of people talking about evals and saying they're very very important at the moment. Um, so obviously all the big names in the AI world are talking about emails because they're kind of important to to AI. Um, and kind of pushing this is the you know the next core skill for product managers. Um, or you know like something that everyone has to understand. Um I think if we talk about like what are eval then probably the you know a simple kind of understanding like what why are they important comes from Haml Hussein and Shrea Shanka and they talk about you know generic metrics like hallucinations toxicity um you know these kinds of terms that often pop up when we're talking about the quality of AI responses are quite misleading um you They're used by everyone, but they're not in any way quantified. And if you want to have an AI system that generates really good output is effectively like high quality, then you need to look at data. You need to identify your failure modes and you need to have yeah metrics that are specific to your application that you can track and uh improve over time. So, um, you know, they are you the only way that you're going to be able to run, uh, an AI product and you drive up its quality, make it better o over time. And I sort of think of as eval as the equivalent of QA, right? So we're all used to QA from a kind of pre-ai world where when you build a feature, you run QA on it and that is a process or a set of tests that checks that you are, you know, making your product quality um you're kind of like meeting your bar for for product quality. QA is a kind of you know works on deterministic systems, right? So it's very easy to kind of come up with tests that um can be proved as like right or wrong. You know, does the calculator give me the right answer? Like there are very obvious answers and logical answers that could be computed to that. When you get into AIdriven systems, then your software is no longer deterministic. It's probabilistic. you can put in the same input and you're going to get in out different but similar outputs. So, evals are a way of looking at the outputs that you're getting and measuring the quality of output that you're getting in a uh kind of statistical way in a uh you know metrics based way um beyond just sort of like looking at it and being like yeah that kind of works. Yeah, that seems fine sort of thing which you know is obviously fine you know if you're running an internal automation or something or you're doing like something where you're monitoring every output um but is not going to cut it if you are deploying um AI systems for large numbers of external users. So that's like the functionally what we're trying to do is like we're trying to improve the quality of of AI and particularly you as you start making more complex AI systems with maybe multiple different um AI calls with very complex prompts with maybe data processing and things in between. Every one of those steps is going to introduce you know variation into the output that you get. And if you double the length of your prompt and make it much more specific, do you know if that actually makes it any better? Well, without some sort of systematic measure of quality, without defining what good looks like, then you're not going to have any idea. So, um that that's sort of like high level what um what what evals are. I sort of think that you know if you are building AI applications then there are a few different levels that you might be doing this quality um you know assurance process quality checking process on and they increase in both the amount of effort they take and also the amount of um rigor or kind of how robust and how how good they are. So at the kind of like most basic level, you know, if you are building something like a custom GPT, um, so something you're going to use yourself, then you're going to do effectively kind of like, yeah, what I call like a vibe check, you know, super quick, you know, informal checks. You kind of you you notice that you're using the same prompt a lot. You're like, wait a sec, I'm just going to create a custom GPT for this. And then as you write the custom GPT prompt, you're like, wait a sec, I can make this prompt a little bit better. And so you stick some extra stuff in there and you don't really test it systematically. You just have, you know, one or two kind of classic cases. You maybe like run the prompt that you were going to run anyway. You run you run the um input that maybe you did last time. You're like, "Yeah, that seems like it's kind of working. It's doing what it's uh meant to be." This is also, you know, really good if you're kind of doing um simple internal automations. So for basic um AI apps or automations where they're exposed to a few people, the kind of scope of the uh the you know the agent or the AI is pretty narrow. Um you want to spot like very obvious flaws and you're just getting going in development cycle. This is sort of where you start. As you kind of build complexity though into that system and and you maybe want to kind of like get it even better, then you kind of need to go beyond that sort of like very very gut feel uh approach. And so if you are starting to think like, oh, I'm kind of semi-productionizing like it's not just me looking at this automation or this agent like there's a few other people looking at it. Then generally you start to kind of get to get another level where you say I actually I I kind of know there are like half a dozen or a dozen test cases that this needs to perform well in because I've tried them myself or other people have tried using them. So I will just have some kind of spreadsheet where I've got all my test cases and I'm going to every time I make a major update to my system I will run through those test cases and see what the output is like and if they work or not. So you get to this of like a small set of test cases where you've you've actually thought about what those test cases are and you're kind of running through them consistently but you haven't really automated the process in any way. Right? So this is kind of like a little bit more thinking. It's clearly much better than than the kind of like vibe evals or vibe testing but you're still quite a long way off something that's that's really kind of robust and metrics based. And so the kind of final level what we're going to talk about a lot um in this call is effectively you know metric based evals. So here we're talking about large scale test suites. So hundreds or thousands of test cases that you've got from live data or you've built from synthetic data. And because you've got so many test cases, you can then score those against multiple dimensions, multiple evals, and you can see um overall like are you passing or failing those those evals and when you compare that across you know a large number of test cases that's effectively going to give you a pass percentage um you know a quantified measure which you can then improve over time from version to version. So this then becomes really important for you know systematically improving the quality of your product. But when you think about the kind of setup required to gather the test cases to design the evals to then build in running those evals into your um application development process. There is significant investment in here. So this is not something that you're going to do unless you really are kind of launching production you know ready uh AI products or your products that where the risks are high it's exposed to lots of users the quality needs to be good for for all of them. Another way of thinking about this um is or kind of like saying that same thing in a slightly different way is you know evals only make sense for complex AI systems. And if I think about sort of um you know workflows or you know software in general like fits on a spectrum from being more deterministic to more adaptive. So on the left hand side of the scale here you've got very simple workflows. Um this could be a you kind of workflow with no kind of front end. It could be a um you know a CRM piece of software or something you know two three years ago when you know pre pre kind of chat GPT where you've just got deterministic outputs now in that case you're doing QA right in this kind of like workflow scenario if you start to add in some AI steps yeah again but with a fairly linear flows with quite narrow um scope for Each of the AI steps you've got what I would call an agent workflow. Then here you've got something where you might be doing that kind of like you know whether depending on whether this is internal external number of users you're going to be at levels one and two that kind of like vibe kind of level of checks or you're going to be at that sort of um light level of of evals. But then if you get to this kind of agent level where you've really got multiple um agent steps, you've got LLMs actually deciding which tools they're going to pull, which bits of knowledge they're going to use. Here you can see that clearly that just the scope for the AI to give you back an output that is not what you want or is not appropriate um to you as a user increases dramatically. So it's in that kind of case you know this case on the right this kind of agent case that eval come into their own. So I say same same way of saying this you know different way of saying the same thing. Um your the amount you're going to invest in emails is going to be related to how complex is the AI system that you're building. So um there's a few different ways then that you can do testing on AI products. Um you can have pretty simple kind of code- based. So even um when you're looking at non-deterministic output uh you can still run you know codebased objective logic you know based rules um to see if the output meets the criteria. So if you've asked your AI to you know give you a JSON output you can test if it's actually kind of valid JSON or you can do a kind of you know a check to say like have we exposed any email addresses or any kind of personal um identifiable information PII here these sorts of checks are you know very clear they're you know very clear true or false they're cheap they're fast these are great Okay. Um, the second type you've got here are LLM as a judge. So, in this case, you're going to ask something like, you know, is the tone appropriate? Is the agent accurate? Is it answering the question? And to begin with, you you're going to look at the response and you're going to grade that yourself as a human. But as you scale up, what you want to be able to do is train a separate LLM. So an LLM that's just judging the system you're building. And your judge is going to kind of grade the the response and say like, "Yeah, it passes or no, it doesn't." And this works because your judging LLM isn't trying to generate the output, right? It's just going to kind of um monitor a single failure mode and it's not going to say hey this is a great output yes you know 9 out of 10. You're basically going to ask it a very narrow question like so you're gonna say hey is the language in this age appropriate for you know 13 to 15 year olds or you're going to say like you know is the um length of uh answer correct something like this you has it accurately um responded using the knowledge in the knowledge database uh and again you might have specific test cases there right so that's your sort of ll as a judge, you're you're you're converting what would be qualitative uh metrics into kind of binary responses. Yes, it's this or no, it's not that. And then you can scale those up and convert those into a quantitive measure. The other thing that you can do and we shouldn't forget is like you can run AB tests um other sort of outcomes based tests on your product. So eval test the outputs of your product like what is the response the user gets. They don't actually tell you like what is the business impact that your changes are causing and particular if you're a product manager you care very much about you know the business impact the commercial impact of the work that you're doing. So just to be kind of clear when we talk about eval be all and end all of designing AI products. I mean in some senses they are if you just want to make the AI product but if you're talking about developing AI products as a commercial pursuit they do not solve the problem of like are you generating the business outcomes that you want you are still going to have to run those outcome based tests um where you look at you know what is the average session length what is the retention rate what is the churn rate what is the uh conversion to pay rate like all these sorts of things and check that actually yes as you're getting better output you are also also getting better commercial returns. So, eval um output kind of measures like quality measures in the same way that you talk about QA. QA is not going to tell you if your product is going to be commercially successful. Similarly, evals will not tell you if your product is going to be commercially successful. So, um, I'm going to run through then how you'd actually set up evals for a product and and you know what the what the cycle would be. Um, and there's basically there's four steps to this. First step, you're going to collect your traces. So, a trace is effectively the conversation that a human has with an AI system. And um you need those kind of those are going to be your test cases uh effectively. The second thing you're going to do is given a bunch of traces, you're going to analyze those you know as a human and you're going to identify what are the errors that pop up. So you're going to classify the error errors that appear. Once you've seen which errors appear and you've classified those, then you can design evals to test for those errors that are that appear. And then once you've got your evals designed, you can run your evals um across your your your test cases every time you change your system. So you can effectively integrate your evals into your uh software development process. I'm going to run through each of these steps in quite a bit of detail. So if this seems a bit kind of high level, don't worry. So firstly, collecting traces. So a trace yeah it is it's a record of what happened during one uh interaction between a human and an AI system. So that includes what did the user input what internal steps did the system make you know sort of like calling tools or processing data. Um, it's going to include the final output that the system gave back to the human and it's going to include like metadata um about, you know, what device were they on, timestamps, all that kind of stuff as well. Um, so it might look something like this. You can see kind of all the different steps here on the on the trace tree and you can see you can kind of click into each of those and see actually what is going on like which information is being passed system to system you know where where are the calls here you can see there's a bunch of calls to GPT 40 mini um so this is something like what are traces going to look like on a you know complex AI system obviously the simpler your AI system is the simpler your traces will be as Now, these as you collect them, you're going to start collecting effectively like happy paths um versions. You know, these are ones where um you know that you'd be testing if you were just like using it yourself, dog fooding it, um you know, doing your vibe coding checks. Like these are kind of the obvious test cases to to include and you're just going to kind of capture them somewhere and put them to one side and be like, great. Uh yeah, this is the kind of test case I'm trying to make work. This is the happy part. Let's just save that somewhere. That's going to be fine kind of maybe as an NVP, but but really quickly you want to start getting live data. So you want to get some data from how users actually use your product. Um because yeah, as we know, nothing quite like live data, nothing quite like contact with with customers to throw out all your assumptions. Um and and this is just going to like show you actually what is going on. what are people, you know, asking your your product and what are they getting back? Is that does that look good? So, um, you want to start capturing live data um, as soon as possible. And if that means kind of releasing ridiculously early with a, you know, a small beta test group, if it means kind of getting everyone in your company using your AI product, you know, do that so that you can start getting, you know, 50, 100, 200 um test cases that you can then use to run your emails against um and to use as as test cases. The other thing that you can do um if you do want to release this publicly but you want to ensure that there's some you know some kind of minimum quality bar there before you release it publicly is you can generate synthetic data. So, I'll go through this in a little bit more more detail because effectively what you're going to do is you're you you need to have some product intuition here, but you need to kind of think about how can or how do typical um user inputs vary. You know, what are the dimensions they they vary across? And I'll explain that. And then you can create kind of the uh variations of across those different dimensions to generate you know um artificial test cases that kind of explore the whole set of possibilities that you could have. Listening to myself say that I realize it sounds very very abstract. So let's let's look at that in a bit more detail. Um generating synthetic data. So the way this works is like firstly yeah warning um this is only going to work if you really understand your domain and you therefore have really good hypothesis about where your product might fail. Right? If you know nothing about your users and nothing about the you know how they might use your product I you know garbage in garbage out this is not going to work. But if you can speak to a few users, if you are a user yourself, if you've got domain experts, um actually you can start to put this together in a fairly logical manner. So the first thing you do is you want to define um dimensions as I say that the the human input varies across. So if we imagine we're building an AI agent to recommend travel itineraries then the dimensions that you you know you might ask for a travel um itinerary as a human you know you might have a different destination types you might have different trip lengths you might have different budgets so someone you know one person might say oh well you know I'm looking for a city break and it's going to be a weekend and my budget is uh you know £2,000 for the weekend and someone else might say, "Oh, I'm looking for a beach holiday. I want it to be 2 weeks long, and I've got £500 per person." Okay, so those are very different queries, but they're obviously both applicable to um uh a travel itinerary agent. So, as you mix up these different dimensions, you're going to create tupils. So, tupils are these, you know, random combinations of these dimensions. So destination type, city, nature, beach, trip length, weekend, 1 week, 2 weeks, budget, mid, low, high. You create different combinations of those. So one tupil then is city, weekend, mid. The second one is nature one week high. And you create you can obviously create then a huge number of these different tupils, all of which are slightly different from each other. So you're trying to create like unique versions of these tupils to give you some variation in your test cases. these tupils you're then going to convert into natural language. So you probably want to kind of use some kind of prompt there where you can say like hey actually given these are the dimensions given this is what we're doing um convert these pupils into some queries. So this first one you know city weekend mid becomes plan a romantic weekend in Paris for two staying somewhere nice but not too expensive. That's what you know a human might write. this is actually a you know I generated this this kind of example query with with an LLM. So that's something you know if you've got then a a long list you've got a 100 um you know uh tupils that you want to convert into queries you can do this at scale and you're then going to you know manually review these as a human and make sure that these seem kind of sensible as inputs that that language seems human that you're removing any awkward tupils that just don't seem like they make any sense you're removing duplicates right so it's not as good as live data But it's going to be it's a really good way of kind of generating some you know a large number of um yeah kind of artificial test cases um which are going to test your product in in sensible ways. So this is why kind of you know synthetic data or how synthetic data kind of makes sense. So having gathered your traces, so maybe you've got some live data, maybe you've kind of generated a bunch of synthetic data. The next thing you want to do is run your error analysis. Now here you'll see I I've only what I'm imagining here is you've got all your test cases here. I've put eight, but let's imagine there's, you know, a couple of hundred. Um let's imagine for each of these you've got the full trace. I've just put the the initial user input, but remember your trace is going to include what you get back and everything else. Um, all the steps that go on under the hood in your AI system. As you look through those traces, the first thing you're going to do as you do your errand um uh analysis is just make some qualitative notes on how it performed. So this has to be done by by uh human review first and should be done by someone who's domain expert. You know if you were going to do this mechanical Turk style where a human gave you this input and then you had a domain expert behind the scenes kind of wizard vos style like giving the output how would they grade how well the AI did? You may well see multiple failures or like multiple things going wrong. If that is the case and particularly if this is like a multi-step process, go until you get to like the first major failure, right? That's the one you want to focus on. Um why the first one? Because as soon as you get one failure, then you know the AI is probably going to kind of um exacerbate that problem, make it even worse in future steps. So there's not much point going beyond the first failure. And you want to write, you know, fairly brief because you're going to have to write it and read it and that's going to take a lot of time. But descriptive notes so you kind of you've got like an idea of like, oh yeah, what's going on? Why is this a problem? Um why is this unexpected? So this is kind of your first set is you start you then have an annotated set of test cases where you can go like okay this is what we got back currently and these are the problems that we're seeing. Now once you've got those qualitative nodes then you're going to start clustering these into groups. So that's called you know you're kind of making a failure taxonomy. Um so you know you might have one failure mode which is the budget recommendation from the AI system is not what the user asked for. You might have a um you know a uh a response where the AI system didn't meet the constraints of a human. So maybe you are, you know, here you can see like test case four. Oh, we asked for a vegetarian item. Maybe it wasn't vegetarian. It included sushi. Okay. Well, that's not going to work for the for the user. So you create these different categories. Um, and you can adapt that over time, right? So you can, you know, if you've got too many, you can kind of like merge them together. If you've got, you know, if you've only got 80% of all your failures in one one category, maybe you can kind of split it up and be a bit more refined about what you're seeing. Um, but you're going to kind of like work out like what are the main failure modes that you see that you want to try and fix. And having done that, right, then you can take a call like, okay, this failure mode that we're seeing, so for each of your failure modes, maybe you've got five or six of these, you want to go through and kind of classify as one of two things. The first thing is, is it a specification failure? as in is this you know you don't like the output but actually what you asked the the AI to do never specified that this was a problem. So you know you're looking at the output and you're like oh wait a sec this is this is like three pages long and and actually as an itinerary I want a bullet point list that is you know no more than 10 bullet points. Okay, great. But if you didn't tell the AI that you only wanted bullet points, then this isn't really kind of a problem that you need to solve with evals. You just need to go and fix your prompt and then see if it still appears. So, anything where you're seeing problems that you just you haven't kind of given the AI enough context, um, fix the prompt first because that might just be a very very quick fix and you don't need to go through the the labor and the effort of creating evals to solve it necessarily. Okay? It's a bit like ask an engineer to build you something and you miss out half the requirements and they build something you don't want. Right? That's your fault. Like you you need you need to give them the right right requirements. So the second type of thing you might see happening is you generalization failure. So you've given really clear instructions and then something unexpected or undesirable is happening. So these are what you your candidates for evals are things where you're like I thought I was being really clear. I can see this in the prompt. I can see this in the code but we're dealing with um you know nondeterministic systems and therefore unexpected stuff is happening for reasons that are quite hard for us to find you know to to to work out why. So this is likely to be a persistent problem for some time and going to have to require us to kind of like work aware at it to improve it. This is where you want to focus your emails. Okay, so we've gone through the first two steps there. We've gathered our test cases. We've worked out what the failure modes are. Now we're at the stage where we're going to design our eval to uh test each of those failure modes. Um, and as I mentioned, there's basically two ways of doing this. You could have code based or you could have um your LLM as as judged. So, for the codebased ones, like these are ideal, right? You want to to use as many of these as possible because they're cheap and they're fast and they're deterministic. They're right or they're wrong. So, if you can figure out a codebased way of identifying that failure mode, then use it. like don't get fancier than you need to. Um, and if you can like you know if you've got a a codebased rule then often you'll be able to write that you know so that it runs every time a user uses the system. It doesn't need to be an eval that is run in test batches. you can basically just kind of like build this into the system and be like okay actually like we the user asks us this thing we run a test um against this guideline and if the guidelines's not met we ask the AI to reprocess it right and we have some kind of um you know way to correct it or we kind of you edit edit the the response and provide some some kind of answer on the um LLM as judge side here you're going to use this for ways where as a human you can be like, "Yeah, I can spot this is a a problem and I can judge this is a problem, but I can't really define kind of clear logic or kind of a a kind of rules-based test that we could run all every time. So here we're going to design an LLM with a very very narrow focus to judge whether the response passes the criteria or not. Is the tone appropriate? Is the agent accurate?" It's these it's these sorts of kind of qualitative assessments, but you're you're designing for a a binary pass fail, right? You're not going to say like, hey, how good is the tone out of 10 because LLMs are not very good at doing that. Like they're okay at doing the kind of binary, yeah, yeah, this is appropriate, this is not appropriate. When you give them like it's not appropriate if it includes swear words, profanities, you know, blah blah blah blah. you you can kind of list out you can give it like really good context on it's appropriate or not. But yeah, creating a graded scale is just an extra level of complexity that's not not worth getting into. You can also um you're going to calibrate your LLM as a judge. So what what do I mean by that? I mean, um, when you set up this eval, when you set up your your your judging LLM, um, you're going to calibrate it by giving it a bunch of, uh, responses and getting it to decide, are they right or not? And you're going to look at those responses as well and also say, does it pass fail? And then you're going to compare your answers against each other. And there's two metrics that you want to look at there. So TPR and TNR, so true positive rate and true negative rate. So, how often what percentage of the times that you say it's a pass does the LLM say it's also a pass? And what percentage of times do you say it's a failure? Does the LLM say it's also a failure? Right? So, those are your two metrics that um are going to kind of tell you how good your your LLM judge is. It's not going to be perfect, but again, you can you should be able to kind of like do a few quick cycles there, tightening up the the system prompt so you get it to a good enough like level where you're like, "Yeah, this is good enough to kind of I trust this like at scale to give me the results that that I want." Um, and again, this is something where you you just as you're testing your system that you're building, you're going to test your judges on a regular kind of cadence uh as well. So when you've done that, you're then into kind of um you can then run your evals against your your um your traces, right? So you can imagine we've got all our our traces here and we've developed these kind of these narrow evals, five, six of them, right? Not like obviously these you don't want to have too many because the more that you have like the less clear it's going to be to you as a product manager where you try to optimize or which ones you try to make better because you're often going to have trade-offs between like if I make eval one perform really really well then maybe it's much harder to get eval 2 and three to perform as well. So, you know, you're looking for you, it's a bit of a judgment call here that, you know, what's the sweet spot of how many evals give you a good enough coverage of your main failure cases, but you're not having like so many evals that actually it's impossible to decide if it's got any better. So, let's say we've got four eval here. We run them against the output we're seeing, against traces we're seeing. And again we're getting binary outputs pass fail pass fail whatever it is we can then work out you know for this version version one what is the pass rate like what is the the the pass rate against the eval um uh that we get so here you can see like pass rate for eval rate for eval 2 is 75%. Great. We then go and change we're like oh actually yeah we see we're not doing so well against eval 4. I think you know when we look at that probably we can just update the system prompt we can change the model we're using maybe that will make things better then we're going to come up with V2 and we're going to run our evals again then you know make some more changes and then we're going to run you know the emails again on V3. And over time, you're going to be able to use these metrics to compare between versions to see if you're heading in the right direction. Okay? Again, you're not going The goal in here is not to get to 100% on all your evals. The goal is to make sure that you are improving, you know, every iteration to make sure that you are making the product systematically better. And um I'd say you're going to run your evals in in two ways. like firstly on this kind of static bank of test cases that you've created. I mean I say static bank like you're going to add new cases to it over time but you've got a standardized bank of test cases that you run your emails against so that you can do this comparison between versions but then also you know ideally you want to run it against live data like on a regular basis. every week you run, you know, a random selection of 20, 50, 100 um user cases because that's going to catch new failure modes you haven't seen before. It's going to catch drifts in user behavior and you know other emerging behavior. Um so two ways you can then use your evals to flag up problems with with the product. If you've got that far, then you kind of onto your final step, which is then integrating your evals into your software development cycle. So, as I mentioned, you're going to like every time you you make a change to your prompts, your tools, your code, like the UI, you then run that variation against your test case. Um, you might do that against each version, you might do that weekly. It depends how expensive it is because like remember each of those evals is it's an LLM call right so running you know 10 evals on a thousand test cases you're suddenly running like a large large number of LMLM calls which is going to start having a significant cost if you're not careful. So again, you're going to have to be a judge of like what is the cost benefit of running your evals, you know, more or less frequently, but you're going to run your evals. You're then going to track progress over time and based on how you see things, you're going to keep it or maybe you'll roll back to a version. Oh, actually that made this eval worse. So maybe we can maybe we've lost something. Maybe we need to go back. Right? So you're but you're into this then kind of fairly typical product um improvement cycle iterating testing measuring uh and so on round round in a cycle. So that's kind of broadly it. That's what I wanted to cover um as the basics of evals. Um I hope that was uh pretty useful. Just just to wrap up with some kind of takeaways. What are evals? Evals allow you to systematically improve the quality of AI applications. How do you do that? Four steps. You gather your traces, you run your error analysis, you design your evals, and you integrate it into your development process. Um, evals are expensive both to run but to design. This is a time and cost inensive process. So bear in mind the cost of what you are doing. And with that in mind, you know, be conscious of what is your budget for running emails? What is your budget for improving the quality of output? Because you are improving outputs, not outcomes here, right? So, how much better do you want to make your output and how much are you going to invest to get that? If you want to learn more on this, we do have a course on this um starting in January. Um, so if you're like, "Oh, this is great, but actually I want to go and build some evals." Then our course deploy an AI agent kicks off in January. What we do over four weeks is we build an AI agent like a real one that is deployed to the internet that is live that you can send, you know, peers, friends, family to, and we then write the emails for them and run those emails over iterations so that you can improve them. This is like, you know, if you're looking to build an AI product that you can put in your portfolio, this is perfect, right? This is going to be like real practical experience of doing the whole life cycle, everything that you need to um you you would cover in a production setting and then having a live artifact that yeah, just stick it on your LinkedIn or whatever. If you're like, "Oh, this is great, but actually having heard about evals, that's a bit heavyweight for me, but I'm still kind of interested in AI." We do have a couple of other courses which are a bit more lightweight. Um, we've got our build with AI course which covers uh AI automations, AI prototyping, and AI coding in four weeks. Um, and you know, if you want to build anything with the latest AI tools, this is an amazing starting point. And it's also great for just increasing your technical proficiency overall. And if you want to build like AI automations, agentic workflows, um so you know, not a full-blown agent but things to help you save time and uh automate routine work, then our course on automating with AI is going to be the one for you. Um with that, I'm going to kind of look in the chat and if you've got questions, I'm going to answer them. I do the chat here. Okay. Um, Sudha is asking how much programming skills is needed to create an AI agent. Um, very little basically. So if you are using a kind of workflow platform, so something like Zapia or um, Relay.app, app. You know, these are drag and drop systems where you can pull different blocks together and you can include AI steps, you know, which effectively give it aic functionality and then all you're doing is you are writing the prompt for that AI step, you know, the same way that you write a prompt for chat GPT or Claude. So, you do not need to have any programming skills to build your own agent. Um, Victoria's got a question. Quick question. Do you reckon it's a good strategy to base a budget for emails based on ops cost saved from not having i.e. actual agents answering phones? Um, I think if you're looking at agents sort of reducing uh human yeah human cost like then that's going to be a really solid way of of creating a budget for your evals because you're going to say look our our system our AI agent at the moment can solve you know x% of test cases and we free up this much time and if we can get that from 70% to 80% then actually we're going to save this much more cost right so on that basis you know you're still talking a little bit R&D, right? So, you're going to say like, okay, well, the the size of the prize is like this much, maybe a million pounds. How much do we invest in evals? Do we invest £100,000 or £500,000? I mean, we don't know, right? We don't know if that additional incremental spend in eval is actually going to give us the uplift that we want. But it does help you calibrate that you're not going to spend, you know, 6 months working on evals and blow half a million pounds on something where the the total opportunity is like, you know, £10,000. So, I think it does help you calibrate, but there's no kind of firm answer for this. Um, Gan is asking any advice on managing evals when LLMs are updated and those updates uh might change outcomes. Yeah, so if the models are changing then um you know if it's a significant especially if it's significant model change you will want to run your um evals against the new model and check that it works. Um yeah, often if you're building a a system that you're going to be using different models for different things to optimize performance and cost. Um so you know you you generally have like some intuition about which models are going to be better for which things. So you going to take your best guess at it and then run your evals and then maybe tweak one or two of the things that you think there might be some variation and see see if that improves things. Um so yes you are going to um test them against your models. Um another question from Victoria. Are there any other strategies for cases where there's no ops cost saved as of yet? Um yeah I think absolutely like if look if you're looking at you can have revenue based cases right where if you are selling um your agent so if I am lovable you know and I've got an agent that builds people's websites and I charge $20 a month um then my emails are basically going to be looking at ultimately like do people renew or do they churn um or can I upsell them to larger you know larger packages you know maybe my $50 package package or my $100 package rather than my $20 package, right? So, you're going to have those revenue based um frameworks as well. But you just as with normal product development, right, you want to be tying back the work that you're doing to commercial outcomes. And there's only so many ways of of getting those. You know, you you increase the price, you increase the number of people you sell to, you increase the retention, or you reduce the cost base. Um Brett is asking questions, right? Um what is the connection and how logically do you draw the correlation between output and outcomes in terms of commercial output and measurement or would you say run evals in isolation purely for model performance? Um yeah, so I I sort of building Brett, you're kind of building there on the discussion with Victoria. Um, I think in an ideal world, right, you're going to be able to find a correlation between your eval performance and commercial outcomes. You in the same way that you would have leading metrics um in a um in a in a traditional product, right? So, if you think about oh my onboarding process, I know that if you used to work at depot, right? So, social fashion um e-commerce marketplace and we knew that if we could get people to make 100 searches in their first week, they had a high propensity to uh then go on and make a purchase and if they made their first purchase, they had a high propensity to come back, you know, month after month after month and make more purchases. I think similarly if you can say like hey actually we know when um users get a good response on their first query on their first trace they have a good chance of coming back and converting then you've got a very clear signal there you can say like okay well we know that users have a successful response when they pass all our emails what is the percentage of user responses that are passing all of our emails and how do we optimize um for that leading metric, right? But al, you know, I think you just got to kind of accept that because you're dealing with a, you know, nondeterministic system, you might not be, you know, you're not going to be able to optimize that um that metric indefinitely. There's going to be some upper limit to that probably. And so, you just want to make sure that you, you know, the amount of effort that you're putting into optimizing your evals then is actually converting into um commercial impact. Yeah. And at some point like it's going to be almost certainly some kind of diminishing returns, right? Like when you start working on your emails, you start working on your product, um you can make some quite big improvements and then actually kind of get into this optimizing phase where unless you're really at scale, it's just not going to be worth kind of optimizing the process any further. Ah, so um thanks David for reposting that. What observability tool do you recommend um or use for traces? Yeah, I I mean I showed a screenshot of of one there. There are you know five or six kind of um tools out there that help you um track your traces that help you run your evals. Um honestly I have not looked at them in depth. Um, so you know, Google that, ask GPT, do your own, do your own research. Um, but know that, you know, there are multiple tools out there off the shelf that will help you run evals and help you um, get good results. If you're building stuff in N, then there is um, yeah, if you're not N has kind of like inbuilt stuff. So some um, some of the automations tools are kind of building it that natively as well.