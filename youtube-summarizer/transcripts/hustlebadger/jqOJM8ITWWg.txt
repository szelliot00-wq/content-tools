I'm one of the co-founders of Hustlebadger. We provide training for product managers and on AI skills. And today we are very uh lucky to have Johnny um Johnny Quark who is the CPO and CMO at MoK. And he's going to talk to us a little bit about how how MoK works as an Aentic first organization, something like that. Johnny, why don't you just kick off by giving like a little bit of an intro um both of yourself and kind of like what's the setup at Motor K? >> Yeah. Yeah. Yeah. Uh so, uh I'll talk about MoK uh mainly. Um so, Moto K is a a fairly old company. I would say it's something like 15 16 years old. Uh it started in Italy um and they have operations in uh France, Spain, Germany, uh and a few other major EU markets. But the general like value is that it provides uh dealerships um with SAS applications and so it's been around for a long time and through that period it's built software during those years and you know that's like through mobile phones through web 2.0 you know, through react responsive website and there's all this stack of like change. And so when I first joined the company, the applications look uh poor. They're kind of old. They're kind of like uh based off old technologies, old patterns, and just not good. >> And so yeah, this AI stuff has really given not only like interesting things that you can build in the applications, >> but like it's completely changed the way we build applications. Um, and that that's probably a way more mind-blowing thing than than the features that you get with AI. Um, and yeah, so Mo K is a perfect example of a company that if you really embrace the AI stuff, it's it's like it's like this. It's like a it's one in a million scenarios where this thing could completely 100x your company, right? Um, and you know, we are not a AI company. We've we we're just we're like a t traditional vertical SAS company for the most part and the AI stuff has now really transformed the company into something different. And so this is like a very rare story because either a you build an AI product today and you're like an AI company or b you don't. But it's very rare to see a company with so much history, so much debt in different places uh completely just go bananas. [laughter] And so yeah, that's that's all I've been doing for the last whatever 14 months I've been here. Literally since the first day I said, "Okay, this is what we're doing. We're all in." And so yeah, so now we're now I'm seeing the fruits of all this and it's absurd [laughter] to say the least. And I can share all that stuff. >> Can you can you give a few examples of what does that mean? How are the teams actually working like dayto-day? >> Oh, it's uh it's pure chaos. um but in the most beautiful organic way possible. So we we literally like four days ago said a script no more sprints. We're just going to f full like go full combine. We're just going to work on whatever is on the like the backline just keep building things. And so so let me let me just give like a sense like we are like every other tech company uh five months ago which is like ah you know there's like uh you have like let's say five applications and you have five products and you got five product teams and you got five product managers you got like a engineering team tech lead you got the whole typical engineering manager and the head of whatever and so you have all this stuff and then everyone's like okay every two weeks it's like a sprint and we're just going to be like okay we're going to do this these two weeks and then at the end two weeks it'll be live and then we got to tell the sales team all this other process or whatever. Okay. So the problem, it's not the problem, but that's like the way every company works. And what we recently more realized is if we get the AI stuff to really work and really accelerate our engineering, what you do is you essentially remove a big part of your life that blocks your capacity to do more work like and you're able to unlock more work. And when you unlock more capacity, what you instantly realize is all the methodologies for organizing team and work was based off the fact that you had X capacity. But if I gave you a magic wand and I said, I'm going to give you double the amount of humans in your company and they're all productive from day one, you would not follow this process. You actually change them to to accept a new velocity you're working at. And this is what is happening in real time over the last like literal three months, but like every week it gets crazier. So the actual setup now is roughly we have way less people than we did a year ago. And we've never actually had to rehire anyone and we are not less productive in any way. And I don't mean that in a generic AI sense. I really mean we don't write less code. We don't release less features. We do more of everything. We're squashing more bugs like at a a compound rate. And so the re and so the the I and I'll I'll walk you guys through how that actually happens and the technical details of that. But essentially the outcome we probably will get to and it's not even that well planned is that for every squad what you would imagine today with five devs a QA a designer a product manager whatever a tech lead yada yada we really think you can manage a application or a product with a developer a p a senior developer a PM and potentially half a designer and we think you can manage a fairly robust road map for a application. Now, it depends on how complex your application, how many but but generally speaking, if you just like let's say um I know I'm going to pick a kind of generic application like a CRM. You could probably do that with like three people and it sounds really insane, but you could probably do that with three people and build any feature that you like and ship it very fast and solve most bugs. And this is 100% from what I am witnessing really really possible. It is such a dramatic and traumatic change and um and maybe there's natural inertia that fights against that but you can see that being very possible. Yeah. >> So I mean so many uh directions I want to go in here. Um firstly like if we just kind of again I want to get a sense of like what this actually looks like on the ground like so what is the tool stack that PMS developers are using? Are they are they all include code? Are they using something else? >> Yeah. Yeah. So we're we all use uh cursor um non-technical teams started to test uh cla work and cloud code to do non-development work but I'm just going to focus on software development. So software development it's all cloud code. Sorry it's all cursor and the default model is opus 4.6 for us. Uh it's the one where we buy the most tokens. It's the one where spend the most that's that's the setup. Um and then you have your like tech typical technical setup. You got your dockers, your AWA, like all that the typical stack, right? Um and the change that we've had to make is we had to install this on product managers and designers desktop which is actually like kind of hard. There's like a lot of steps to get a person ready to develop code, but that's been kind of interesting. And so essentially cursor is the main tool and really that's what we use. uh some more details. So cursor of course you talk to it, it writes features, etc., etc. But we also created a skill to write tickets and that sounds kind of stupid because like you can just like write a ticket and have another LLM write it. Why cursor? Well, the beauty is we create a skill that lets you write a ticket in cursor and cursor just like bruises through your codebase and writes a much better ticket because then it's like it should be like this because this dependency and we're like what like how is this happening right now? I mean this is like mind-blowing and this is all stuff we just like created internally you know we didn't read. So it's really like just the team just unleashed their brain on this and just like started doing all this stuff and now since the last two weeks we now try to write tickets in cursor we don't even write in Jiraa we're just like bypassing because it's much better it's more detailed and plus the tickets writing is going right back into cursor. So the amount of details really matter and the context is much better. So we we're finding like oh there's even more ways to do it as well. Um and as of three days ago we implemented a Ralph process. I don't know if you guys are It's this viral thingy. Oh, this is crazy. Yeah, this is even more crazy. Like literally every week there's like a new thing. Uh but anyways, I'll pause there because there's a million and one tactical things that are so impressive. Um but yeah, wherever you want to go with that, I can give you more details. >> So yeah, I mean um just talk us through then the the development process. You said you've got rid of sprints. It sounds like you've got rid of Jira or you're getting rid of Jira. You've gone from teams that I how big were your teams before? Before they were like, >> well, just to give you a sense, like we probably had like three squads under one product. Now we have two squads. Uh I think you could have less. What what you like generally need is less people that but like a few more senior people, right? And what you're trying to do is have less humans write code, but more humans design software, which actually is very hard. And you still need very good engineers. And that that is something you can't cover with like AI. like you need a good software designer and I mean like designing the application, the databases, the efficiencies, understand like you know that plus a product manager um designers is kind of optional and I'll explain why in a moment. You need like a few but you need a lot less than before but really uh you need a good software designer and that's actually a very hard skill to hire for because usually means they're very good developers. Um but yeah, this the teams are just smaller by like at least 30 35% on average if I was to give a like a broad answer. >> Yeah. And so and so what's the software development process like the the PMs are I mean are you doing any kind of planning cycle? Are the PMs writing specs or like >> um well I mean I don't know I guess maybe it's it's not that different for most companies but for us we're you know in the current moment we're inundated by a huge backlog and because we're B2B a lot of the backlog items come from our biggest customers which is very hard to balance as you can imagine like any company. So our kind of focus right now is like all right can we cuz we're getting faster and better and more processed with the AI stuff. I'm like can we clear our backlog like can we clear it and clear all the bugs? And the feeling I get is we can probably clear all our bugs like very fast like maybe in the next eight weeks we can clear down to zero bugs across every single product. We we think that is possible with this process we developed. Where it's harder is so our processors design features is like in a fortunate or unfortunate state we have well-defined problem statements and features that we need already from the past and the problem is we've never had the capacity to work on them like like every team. So now it's like okay we we we have the capacity to work on all of these things and it just depends on how much attention you give to it. Um, so right now we're it's not like there's not like this like roadmap per se, but this is like, okay, here are the most important customers. Here are the requests. Let's just go down the list. And I think the difference is like there's just way less planning because we're just building it. But the planning goes into the feature or how the feature should work or how a feature works in the back end. Like those are things we're working on, but we just don't care about like schedules. We're just like completely void of this concept right now because we think any schedule you can come up with with the traditional method of software development, you're like planning quarters and months and giving dates to sales teams, but we believe that actually it's it's very obvious that it's like it's almost like 10 times faster. So we might get to a point where we just don't even have a backlog. And I realistically think that can happen if you don't collect any new things in the next like whatever few months. So the process right now and I would say it's just a lack of it because we are just making everything like go. We're just like deploying things constantly. We're doing so many pro requests a day. We're editing like thousands of files. So right now it's uh it's really like forward momentum. Like the most wildest it's just crazy. >> Tell us a little bit then about the transition because you said you've been with the company just over a year. I mean like I imagine there's a kind of like certain amount of momentum you got to build and then it all kind of becomes a little bit easier as everyone gets on board. >> Yeah. Yeah. Uh I think first there's like a general feeling that okay AI is important and we should all have like learn how to use it our workplace and then when you kind of like start distributing these tools and teaching people how to use them then you realize that most people look at AI and think of it like software. So then you you expect software that when you press a button it does X. And so you press the button 100 times so it does Xund times. But like AI doesn't work that way. You actually it's like you have to press X then Y then a little bit more X then tell it you know like it's a weird it's not like software. And so that's the first thing you learn that non-technical people don't manage it well. And so then what you do is like you waste all this time to get everybody to use it. And then you realize okay I'm going to focus on the highest leverage point which is code. And so then you go okay we're going to focus on making our engineering team use AI. And that's very hard too because like engineers they're all opinionated and they have different opinions but like but like half of them are just like h it's not good enough it'll make mistakes I can do it faster myself you know you have this kind of like reaction and that lasted probably for like 70% of last year and towards the last quarter of last year more and more people were like actually this is kind of faster and then prior pretty much like end of November it's just like the like Gemini 3 came uh Opus 4.5 came out and it just made way less mistakes like every time you ask for requests and then it became like the people that were like this is good to like this is all I'm ever going to do and then the people that were in the middle were like like this is way more productive and now we have still a percentage of developers that are not using it day-to-day but we are immensely like we're moving towards that like there's no one who is not going going to be using only AI to write code going forward. That has been very hard, but the tools just naturally become so much more convincing over time. Like you you just cannot deny how good they are these days. >> What were there any significant blockers like just organizationally to kind of getting this level of transformation and like what were the biggest ones? How did you get through them? >> Yeah, I would say there's two. Uh there's organization and there's [clears throat] technical. Now I talk about the technical one. is very actually important. Uh it's actually the excuse you will get from most teams why they don't use more AI but the operational one um no I don't think there was like a huge blocker but it was like number one you got to get like everybody licenses the cursor then you kind of give them like some light training and then you got to set up all the environments and then what you had to do is you have to define how you have to define rules around how your engineers will interact with cursor so that cursor becomes comes a better document of what it's doing for everybody else in the company. And so we created like what we call a blueprint, but you can call anything else, but it's basically a step of how you should use cursor. And that says something like, okay, give it clear requirements, didn't do this, didn't say that, then make sure it updates this file in this context document for this project so everyone else gets that context. And it's just like this um like hygiene, which is basically what a really good developer should do, right? like they write code, they document it, they make clean notes, it's very straightforward, everyone knows where that note is, so the next guy can, you know, so it's the same thing, but you're trying to distribute that as a process within an application for everyone to follow when they're using it. And that we had also designed in house um it's it's like I guess there is a lot of content about this stuff, but but like you had to experiment to and we we have our own quirks, our own systems where our files are. So um so that was the first operational pain is coming with a a process on how you use that. Um but >> that's a skill built into cursor then. >> Yeah. >> No, it's actually a set of instructions for the human. [laughter] It's a prompt for the humans. Uh and the prim go okay did I uh make sure to tell cursor to make sure to update this file at the beginning of my session? Did I make sure that cursor knows that this is the context? that I make sure that cursor update A, B and C and you know like kept notating in the same structure that we have all the notes. So that way you know fast forward in the year uh someone can just be like gee I wonder what everybody worked on here for the next year you could like press a button and see it as and that's like the holy grail of software development because no company has ever documented well and so no one can ever do that but you can really do that now like you can really do that. So this is like such a unheard of value that like just you don't imagine but you can document everything perfectly in good clean writing that's accurate and updated on the fly >> and and and then is it humans who are kind of providing that link because like curses working locally on your machine I'm imagining all your kind of context strategy documents are all like shared documents in I don't know Google or teams or whatever you're using like so is it humans kind of copying stuff backwards and forwards or >> no no no it's no it's in the project folder so if you're working on like product A product A has a folder you have a repository everybody clones it you update the files that gets distributed back you know like the it's like any other application so you put it in the application file yeah >> but then does that mean that non-technical like non-product stakeholders marketing ops have to pull that that repo as well to get the context >> yes that's correct uh I mean someone else can pull it for them but uh but they don't like the individual doesn't need to know it it's actually for hers to load up and go Okay, this is what's been done to this application. So, it's not about the human knowing it. It's about the the AI knowing the context of the application. Which leads me to the first and biggest technical blocker, which is context, which is the word you will hear a billion times with this stuff. So, the first issue is like you tell tech teams to do is they'll be like, "H, our codebase is so big and there's so much tech that they won't get it. It'll make all these mistakes." And they were right 14 months ago. They were 100% right because it did not work at MK. But then you go back six months ago, it's like kind of works on some of our small applications and now it just like works on any application. Now our applications are poorly built, kind of old and very big. And so if it works on ours, pretty sure it can work on any application. So now what we've discovered is a few things. One, it doesn't need to know everything perfectly. It's actually quite good now filling in the gaps. So it understands it finds dependencies better. So that's good. The second thing was we did have to do some refactoring of the applications to make AI more effective on it because you can imagine if you like toss someone like a really messy application with all kinds of spaghetti spaghetti code the most smart person in the world would make mistakes. So we're trying to prevent that. And so the first thing we did to make life a lot simpler across the board is actually the front end. So we built like a design system which is not like some innovative idea. A lot of people use design systems but we designed we built a design system and then we made all the applications use that as the main component library. And what this does is number one, of course, you get unified UI across all your applications, which is nice, but it also allows for any product manager or engineer to build any feature without a designer because the AI will just use the design system and build the UI you want and then you can make edits or whatever. And we found that this is amazing because number one, you get consistent a UI, you get mobile ready, you get responsiveness, and you keep you're able to keep growing the design library and add more context to all the, you know, like this is what a card is, this is what a table is, and it gets better and then the AI has better things to pull from. And so you're at so it's just like this compounding value thing. And that was our first big like commitment to get AI to work is to say we're not going to write front front-end code at all. We're just going to pull from the design library. And that was our first like we're going to make this work. And we just like it took like six weeks. And now it once it's implemented, we're like holy crap, this is this is like unbeliev code again except to actually never. And so now our designers are able to uh spin up features uh change buttons in the application do poll requ like this is all completely possible and the PMs are doing it as well. So this was the first evidence that we're like this can really work at scale um is actually the design system with the front end. >> Okay, super. I'm just going to ask one more question before I kind of throw it open to the floor because I'm sure there's a whole bunch of questions from people here. um what has been the the headcount implications? Have you had to let people go or retrain people because you're talking about changing the ratios in the teams quite dramatically, right? Um so what what has that meant in just terms of the organization? >> Yeah. Uh I don't like I can't say like there was never like I wish there was less people here because we just have AI. Like that's not the statement, right? The statement is more like, okay, we're going to like be a company of a bunch of people that really want to take advantage of these tools and there is a percentage of your company that that won't do it, doesn't want to do it or whatever. And so over time, uh either they'll leave because there's better jobs that are more fitted to what they want or B it just feels really awkward to be around a bunch of people like coding and AI and you're not coding AI and that that has some natural attrition to it. So that's kind of like what's happened. And then what you realize and it's not like we had like a for like we didn't have the foresight for this but what you realize you don't need everyone you just need the most senior contextrich individuals in the company and that's all you need and so that number might be smaller that number is definitely smaller than what your current company is for sure. So I'd say it's like a side effect to have less people but it's not by design. It's just like you realize you don't need many people to design good software if they know the customer really well. They know the software really well and they know what they're trying to do really well. That is it. If one person can do all that, that's all you need as well. So that's kind of like uh where we end up. But it less people is not the goal. It's just like you realize you need you need less by accident. Yeah. >> Yeah. Yeah. Super. Okay. Um I'm going to throw it open to questions. Um maybe people can just use the uh the the react button down at the bottom. If you pop that up, if you click on that, you'll see a little raise hand button. Um just so that we can kind of have some kind of order and then I'll I'll I'll facilitate. Um so, uh Tama, >> hi Johnny. Um this has been great. So >> two questions really quickly um before Nick. Um so the first was just you specifically were talking about that first milestone on the consistent UX and UI and sounds like platforms in our situation is pretty similar. One of the big things we actually don't have a front-end uniform at all and so we're in the process >> right? Yeah. Yeah. Yeah. >> Yeah. So so was the purpose for that actually just to do the same thing we're trying to do to actually get that consistency or was it just a test like you're just like for all new code we want to have the same UX UI. Um were you was it like me who I'm trying to get that consistency across the platform or was it just moving forward we need to have consistency? >> No. Uh I I think there is a desire for me personally to be like oh I wish these products would just like work the same so our customers don't have to change the context every time they're switching like that's just kind of annoying. I would like that. Is it the biggest problem in the company? Probably not. There's like way bigger ones so we never deal with it. Um, and we were in the process of redesigning one of our products that we feel like is our big revenue driver for like 2025. And in the process of redesigning it, uh, you know, we were using all kinds of libraries and all this stuff just to play with the UI. And I was like, oh, I was like, wouldn't it be great if we just like switched all these over? And everybody was like, yeah, that would be great, but you know, like first you got to design the design system, then you got to get all the components, which is okay. you can download them and edit some and then you got to like upgrade all the front-end code for all the applications and then get them onto this like you know library and if you just kind of like roughly add that up that's like a lot of work it's probably like six months worth of work in total for every for like five applications but then fast [snorts] forward to like a few months ago we're like well can AI do just everything I just said uh I don't know let's try and shocking it can. So it was not about consistency for us. >> [snorts] >> It was about we were resigning one product that led us to do a design system because at some point we wanted to do consistency and then that accidentally led us to oh wait we can have every product on this now not six month like literally today okay and then we're like with AI and that's when we that was the moment the moment that we got one product on design system and we're able to tell cursor hey I'd like to change the login screen so it's like got dark mode. Boom. I'm like, everyone's like, "Guys, all the products this month, we got to get now." And that is when we realized the AI stuff really freaking works. So, consistency is just like a side effect value of what we were trying to accomplish in the first place, which was just designing this one product. And then that led us to like AI can do this. And that led us to like let's do all of it. And literally today, as of right now, we are uh refactoring the front end of one of our applications. Ahi AI has rewritten 80% of the front end right now and integrated in the library and I can see it like all the buttons. I was like it I was like sitting there you know my my uh head of engineering for that team said it would take four months. It's taken 4 days. We will be done in two weeks. We'll have a completely new UI consistent on an application we thought was going to be an H2. Done deal. like absolutely mind-blowing. So it was a accident I would say to get consistent uh user experience. >> Amazing. I'll get back in line because um and then I'll ask my second question after. Unique >> did you want to jump in with your question? >> Yeah, I was just uh just saying I think Johnny just answered my question in the previous answer about the sequence of implementation. if you started first with a small team or did you see the whole uh or did you need to do the whole company to see really the efficiency of work? I'm asking because I'm working for a huge company. >> Yeah. >> Huge. And if I want to implement this, I need to start somewhere, right? We can't >> Yeah. Yeah. I I think uh for me um I'm not sure how I would approach it, but certainly it it naturally has to start with a small team because when you roll it out to all the teams, you have to have a better defined process. And we only rolled it out to all the teams when we defined our blueprint or whatever you want to call it, which is basically our process. But that was through uh maybe like five weeks of hardcore like trying this, trying that and then having a few people to come like passionate about it that they had their own u perspective of how to work with AI. So then we had those people really write it down and then we put more people into the group and they evolved it and now we have a process that we we like and we'll probably change more of and that naturally becomes the default rules for all the teams and then you can onboard the teams but but to onboard all the teams to try is a disaster. It's a complete disaster. I wouldn't do it. And also, if I would suggest what I actually would focus on the front end because it's the thing everyone can just see. You're like, you built that? Like, you really built that and it's live. This is all you got to sell. And then everyone's like, holy crap, we're going there. Yeah. M >> um can you talk a little bit maybe about some of the guard rails that you had to like if everybody can change everything all of the time um all at once how do you set some guard rails so you don't end up with a Frankenstein thing or that someone accidentally deletes your entire [laughter] >> please don't do that well I mean I guess an engineer could just do that now right so you have natural guard rails that protect that so that doesn't happen but uh and it's not Like most of the people working with cursor are engineers just to clarify. It's not like okay everyone in support is using not that yet we are experimenting with that for bugs but not yet but right now it's mostly engineers all the product managers and every designer which is you know it's like like 30% of people are not engineers. So most people know what they're doing. Now insurance and guard rails all this stuff. The thing is once you make all these changes, you have to do a poll request and you know the the natural uh the natural blockers to your productivity will then be the bookends. Did you define the problem well and the feature well and did you the work that was done by AI and to make sure the feature works and you you just like fly through this middle part now but the first part is still human work and the last part is still human work. You still have to review the code, review all the pull request, then you have this huge queue of pull requests, which awkwardly enough there's also AI to review that and then you review the parts that I didn't review. So that's kind of the next bottleneck is finding a way to uh review all the poll requests. Well, so there are natural uh protections but more in the traditional way of doing software engineering. So the AI cannot bypass what an engineer uh can't bypass. So you don't have that issue. Um it's not that bad. Um and most of the people are editing uh local files. They do a pull request and if it's good and approved by other devs then it gets merged. So so no uh there's no issues like that. Um in regards to like security and APIs and all that stuff, we have rules uh that we build into cursor that kind of follows that. Uh but we also just have your typical enterprise security stuff. So I mean if a software engineer can destroy your company, so can AI. So far, we don't think a software engineer can destroy a company. So, we don't think AI can do it yet. [laughter] >> My my brother-in-law who works um in the States at a listed company and they went AI first tells me like every month like how his life is changing and he did tell me a horror story of how AI started deleting their data files. So, >> that's crazy. >> Very terrifying, I'll just say. But, um >> yeah. Oh, I would say also just just on that like that's actually a very good point. If you're doing AI like AI engineering like databases is very you have to be very good at it yourself. So I as a product manager or product person I would never build a feature that may had dramatic database changes. Actually I would avoid that entirely. That is still for a senior like I said you have to be a good software designer to work with AI. Well I am not a good software designer. I'm a I'm a buttons and link guy, you know, I'm like this stuff. So, I don't typically if there's a future that requires that, that is actually when you need to involve a senior developer to be like, how should I plan this with AI? Um, so yeah, typically avoid any crazy data structure changes. This is no bueno for Johnny Quark. [laughter] >> Johnny, um, I'm curious. So, I'm in the legal tax space and our customers are very riskaverse. the data is heavily regulated. And I'm just thinking I we don't have any blockers from the board or my executive team on going like full-on AI. So, that's the good news. But I'm just thinking if you put yourself in my shoes, like would you approach things differently in any way? Would you um do you think there's things that I need to be more mindful of? Like I'm just curious. Um, I think if I think your engineering team, so I'm I'll make assumptions, but I think your engineering team up at this point before AI probably developed an application that has security in mind and risk adverse built into it like and I feel I feel like their software development process also has that in the DNA. So if that is the case, I mean cursor is just like a very cool coding application. So it it's not going to go bananas if you don't allow it to go bananas. So I I I think that's kind of like the main thing. Now there are probably more technical people that have a better answer that that understand the security part better than me. Um but from what I can observe, you know, as long as you're staying away from infrastructure changes and things like that, like you're not going to do that poorly. Uh like nothing would go horrible that I can imagine. Um, and I mean security is like obviously important. Um, but it's just it's just such a cost to not do this. Like I can't even like elaborate how a loss of money it is to not do this. Like to to not at least have a percentage organization just working this way in the next like it's just like it's like suicide. I think it's so bad. And I think most companies don't do it yet but I'm just would be shocked. Yeah, >> I would be shocked if if like Yeah. >> Cool. Thanks. >> Right. Hey, Johnny. Um, in the spirit of learning from your experimentation and failures, what what has gone wrong? What mistakes, failings, screw-ups have you had along the way that we can learn from and avoid in our learning journeys here? >> Uh, you know, it's like unfortunately, um, no, unfortunately, but a lot of the mistakes that, okay, so there's a lot of mistakes we made. I'll go into a few of those, but a lot of mistakes that AI made was the models were not good enough to deal with our poor software design. And now they are better. So, a lot of problems, as you can imagine, just start to disappear as the foundational models get smarter, which is insane to me, right? Like, you didn't do anything, but it just got better at dealing with you. Um, in terms of like um failures operationally, it's like I I I I thought I was very strong about it and doing this, but thinking back, I was not strong like I wasn't even I wasn't directing enough. Like I was saying this is important and every in my executive team was like yes, but like you have to force people into it and that sounds absurdly negative. Um but it's like like forcing the engineers to use it which if you go back even a year and a half ago people were online like oh my god my boss is making me use cursor what an idiot like you see all these comments you don't see those comments anymore like they don't come up that sentiment is gone like it is absurd not to really use these tools so I would say that I wish we were even more crazy even though they didn't work that well back then but now that I'm really seeing it it's just like man our our bet was right and I just wish we accelerate even faster. Um, and so that's just like just like on a on a high level on the on the more personal and this is for engineers and non specifically non-engineers. It's like I I often over assume like like everyone I think but often over assume what someone knows because I know something and so working with is like very natural to me because I'm just like okay I get to tell it what to do all the time. Fantastic. But most people are not good at that. Like most people are not good of giving instructions and I just expected people to be like, "Wow, I have something that just does anything for me. I'm just gonna use it all the time." That is not the case. [laughter] Most people just stare at it and like what do I do now? Like unbelievably like the more people I watch use AI, they just get stuck because you don't know what there's no cues, there's no buttons. So that was the biggest mistake is um if you give a bunch of people a bunch of AI tools, they actually don't know what to do and you just have to treat like any other software. There's some training, there's some inspiration, there's some guides, there's some follow along activities and all that stuff. But I kind of try to bypass all that because I just felt like AI is so natural. It's just like a human just talk to and it does stuff. Absolutely not. It's literally like teaching someone how to use a computer from day one and you should not ever assume people get it because they don't get it. Like I it's mind-blowing for me because I'm such a nerd at this point, but lots of people just go, "So what? So what should I do now?" I'm like, "I don't know." Like do your taxes. I don't know. They just do something. So people get really stuck. And that is true even at work. [laughter] >> Yeah. Strongly agree with that one. And as a experiment myself, it's something I'm also struggling with. So cheers Johnny. >> No problem, man. Uh, Emma, let's go over to you next. >> Hey. Um, yeah, I wanted to ask a question about that kind of point that you made where you have a feature that you know is kind of getting to the point where like you need an engineer involvement. You want their input essentially on the kind of spec you're writing. What's the process that you guys use for that process of like defining a spec? >> Yeah. Yeah, definitely. Definitely. Um so um with any if if you like read any content on like people who are like very good at using AI to code like uh and anyone who just uses a lot the the general uh sentiment that comes with it is that you should plan a lot before the AI writes code and you can do that with humans you can do it with the AI you can do a combination of ways but basically you're trying to create so much rich context that it comes up with a good plan the problem there is if you're a engineer you're able to read the plan and go that is stupid or that is very good or that's mediocre if you're me you're just like fantastic there's a plan let's do it right and in most cases I don't even go past the first sentence so the way we think this is going to happen and we're going to formalize it more in a week is actually just to do pairings so senior developer and PM and just are constantly working organically together and like I said we kind we're going to get we're just getting rid of all the schedules and the meet like we're just like okay just build here's a long list of stuff to build just go and so it's like a little unstructured but we just have a senior developer with a product manager just working together consistently um and we think that will solve a lot of problems because the product manager knows the customer problem and the feature really well the developer knows the intricacies and the weirdness of the application and then you have AI to do all the grunt work right before but I would say if there's one thing to state about this whether we're humans or AI you need to plan the features in detail and go back and forth and really create a robust um PRD which I don't ever even like writing but but really that is extremely useful to oneshot uh success when it comes to features. >> Yeah, makes makes sense. Um, I also wanted to come back to a question that I think Maud asked about guardrails. Um, because I have just spent the morning fixing 10,000 lines that our UX designer wrote on our website. Um, and say the most important guardrail is is small PRs. Everybody needs to learn how to like coach AI to write small PRs because I reviewed that PR. I I was like, hey, I think this thing is wrong. He was like, Claude tells me it's fine. I was like, great, you you go for it. and then like realized afterwards it was actually like, you know, it messed some things up. But if it had been more like 500 lines, I probably would have caught it in more detail. So, >> no, that make that makes perfect sense. And um I'm like I guess that can also happen with us. Obviously, the we just have more engineers reviewing that stuff. But um even with this problem, there are tools that are getting better at at dealing with that, like to observe that. And this is where it's just like Jesus. Like I'm just going to fast forward six months in my mind. It's like what problem are we going to have left to do? Like I mean yeah like what only capturing the customer problem like that you know if the QA gets better the pull request the quality of code like it's none of it's getting worse. So this is like strange. So you like so maybe all the value goes to how well you know your customer and how well you sell or how well you capture the feature. Like it maybe all the value goes right back up to product management. And that is crazy because it's it's very foreseeable. Um so yeah, I think there's a and I think we use code rabbit and there's something else but we use those to kind of do more checks. Um but even then um even the even the companies I know that are like way more AI native they still review the code humans. So that's still a thing you have to do for sure the poll requests. >> Yeah, makes sense. Thanks. That's actually what my question was going to be about like for your engineers now how they spend their time becomes a lot more about reviewing pull requests rather than writing code and I think for a lot of engineers that's the boring part how they like how do you h have you thought about hiring differently or how do you manage that >> h I don't I don't have a good answer for that I you know it's uh it's all within the last six weeks it's becoming more of a problem because there's just more work being outputed than there is work that is being released. Um I don't know to be honest. I really don't know. Um but I guess you you'd had to at some point in the future of your life do these hours so it might as well be now. [laughter] Um yeah I don't have a good answer to that. >> I guess the AI is going to solve some of it. like you were saying, you're using AI to do some of the parts of the PRs or like a preview pre-review or >> uh we have uh we're using code rabbit to review some of the code on like bigger code bases. Uh and it does a good job, but we still double check its work. Um but it does do a good first pass. Um and I'm like 99.99% sure there is a company being built or built that is very good at this already that would just get better in the next six months. Like it's I would be shocked if if there wasn't. I mean like literally I would build that damn company. Oh my god. Like no clear. >> Amazing. Um any any other questions uh for Johnny while we're here? >> Pricing. >> How are you uh maybe I don't know Johnny I'm just thinking so I don't know how your exact team is wanting reporting on this progress pricing anything like that. How are you managing keeping your stakeholders clear or your board or your investors you know clear on what you're doing? >> Yeah. Uh so it's good really good question and uh it's going to be like my answer is going to lack a lot of details but essentially um all those things mattered a lot more if you had like a one-year plan and at this rate we believe we can accelerate the road map by like more than half. So, we're just like in the most funny statement ever, it's like we're just ignoring everything except building things faster right now. Like literally, we're ignoring all of it. Uh because it's just all waste of time because all those things existed because you had long timelines and you had to keep everybody in check. But right now, we're just like, okay, we think we can just release these things much earlier all at once. Um and I give you just one like visual problem. Um, we have like an application right now that has 53 bugs logged against it in the last like three months or something and it's it's it's a a really poorly built application that's very old and it constantly spews out more bugs and the more customers it's just like a disaster and we try to keep it under like 23 bugs in the backlog every month but all like in in the recent months it just went like bananas because more people were using it. So um 3 days ago we said all right guys let's just like let's just let's just deal with these bugs like this is bad um okay guys focus only on bugs right for whatever period and they were like oh like two weeks so this is day two so day zero was let's do that and I said guys leverage AI to solve the bugs okay everybody's using AI the the the head of engineering for that team is one of the star players for AI And so there's this thing called Ralph. um if you guys are like chronically online like me, but uh Ralph is this um set of instructions for AI coding that basically goes here's a list of the things I want you to do and here are the features and this is how you QA and you just write these really good requirements and you create this like long list and the theory is that uh AI will go do it go back and check against the uh uh requirements and it just keep doing this in a loop for 24 hours. And the idea is that if you have really good acceptance criteria, it will be able to accomplish it and it'll keep doing it until it hits those criteria uh acceptance criteria. And apparently this this process um which actually devoids it of context every time because it updates the context for what it needs to know the next time and it starts over. this apparently because the problem is with AI if you give it too much context with too long of a window it like becomes like very bad and dumb and so this actually does the opposite which is reduces the context in loops so it's like tiny tiny tiny loops and it goes in circles and it keeps checking things and it keeps doing it until it's done. So we set this process loose on our bugs and of the 53 on the second day it's solved 20 of them by itself. So this is like uh crazy cuz we were solving like one bug a day per developer. We're just like what the hell? So we're just like super blown away. This is like crazy stuff to us. Um but yeah, so that's kind of like uh it's having such a dramatic change on our scheduling. Um we don't even know like what to do with the rest of the company right now. We're trying to figure like, oh my god, like uh should we plan more? So, yeah, but it's it's a that is one of the most tangible things I've seen it like like what this is crazy. We're trying to keep solve 10 a month and now it's like bloop 20 one day. We're like oh god. Yeah, we're all in danger. [laughter] Um, I was curious how your clients have uh like have they noticed the change of pace and how are they reacting to it? I think they're going to feel it more in Q1, Q2 because like I said like in the last three four months it's like ramped up more and more and over the Christmas break uh like the whole world just went bananas and I was completely in a cave like just working with this thing and I was like why is it every engineer in my company is not doing this like I'm sitting here working on four applications at once constantly talking to my computer like just use not even typing justah blah blah oh the button should be here know the feature I'm just and I'm just like what is the rest of my like what is the company doing and so I was just like no we like there's something to happen like the models got better and all this stuff and so now we just basically all of January was just making everyone do this um so I think the customers are going to um it's crazy like our applications got faster in the last like four weeks because we like found all these stupid things with cursor um and now we're starting to actually release like new UI to customers in the next like month So, uh, we'll know in like a few months, but, uh, yeah, I can't be negative. No way. No way you'd be negative. >> Um, small question for you, Johnny. How big's the team you're managing around here? Just to like understand this, obviously you've like done that shift, you said since like December to February 12th. >> That's a pretty fast turn. How how big is that? Um so uh when I started here in November 224 uh there were about 20 people in my marketing team and probably something like 40ish people product managers designers like a mix of that and uh engineers were probably I want to say probably 70ish something like that. Fast forward to now, uh, I have two and a half people in marketing, uh, doing the same output, which is crazy. Um, and probably have like 10 product managers and two designers and I had eight designers when I started and no change, zero change because of the design system. um that and you know so so the design system really I mean you need a designer still of course to be like oh you know it'd be better if it worked this way but just to design a screen before you build it you don't need that anymore it just builds it and what we now do is because of that we're able to then take the design library give the current screen and have first spit out a context of what the app is copy paste all that into Google stitch And then Google Stitch pops out a UI design and then we take that UI design, put it back into cursor and it implements with the design library all put together. So we're just like, okay. So the designer, the two designers we have, one just maintains the design library and does user interviews and updates it and adds more components over time and the other one is um jumping around different products, working on different screens, but essentially all of it is built. He's just editing it and moving it around. So the team is dramatically smaller but once again not by design purely by happen stance. Yeah >> Jonathan you hand out. >> Thank you. It's thanks. Uh been fascinating. Can I ask a little bit more around the roles and responsibilities? So I've got more of my uh product manager hat on. So you touched a little bit earlier on um PRDS and tickets and with the role of the PM. Can you just tell me a little bit more about who's doing what? So you finding it's the the engineers are writing the PRDS with with AI and obviously the tickets and the PMs are a little bit more in the problem space, more product briefs, that sort of thing. >> Yeah. No. Uh no so so the the responsibilities are more or less actually still the same right um it's uh so so the the PM still of course uh goes how well is the product doing uh are people using it correctly is this customer successful of it you know all the typical things you would look at your KPIs your northstars or whatever um and then they have features that they're wanting their customers wanting them to build or features that they think that would be useful or whatever and so they still uh do the work of going what are those features and how should they work and then from there they use AI to write the PRD but based on a lot of the work they've already done and then from there for any of the features that are fairly small that don't involve uh database they can actually just go and start working on the features and building it and then do a progress now for the engineers they're not really they're never writing the PRD um we actually just added a skill incursor for the PM to write the PRD with their context of the problem which then makes a very pretty useful PRD for AI so it makes less mistakes. the engineers take that PRD uh and if they're the ones working on not the PM they put it into the you know give it to um cursor actually they just reference the ticket and because we do this we have this MCP connection with Jira or whatever um and it references the tickets and then we have it go into plan mode and then it plans the feature based on P and then it executes so that's roughly exactly what happens on a step-bystep basis yeah I skipped the design part but uh I can yeah that's similar to what I earlier. >> Thank you, >> Johnny. Good question for me. How how much are you spending on credits uh engineer each month? >> Uh like way too little. [laughter] I mean, it's just way too little. It's insignificant. Like it's honestly completely ir right for the whole year so far. And hopefully that will change. Um but at the same time, the models just got cheaper, which is like what is going on? It's all going to zero. I mean, literally like uh this DLM model came out today and apparently it's better than X and it's like 90% cheaper than and fropping. We're like I don't even know what to do anymore. So, um no, it's just don't no one even cares. Like honestly, no one cares. Like spend thousands. It doesn't matter per per employee. It it's all irrelevant. Yeah. >> Ed's question made me think of something. And Johnny, a lot of what we talked about today is using AI on how you guys build as opposed to AI being in the features for the customers. But one of the that we are working on right now and that my team keeps um freaking me out and I keep pushing back and saying like show me the optimization on this, but is one of the features we're working on is with graph rag versus rag and the cost or forex. Have you guys gotten into doing anything with that at all or >> No, not specifically that, but we did have a thing. I forget the exact name. It's called like what do you call it? I think it's like cube or something like that. But essentially, our customers have a requirement to do reporting. And historically, it's a very stupid page in most of our products because every time a customer is like, I would like to see a chart of X, we would have to like go and like have a developer like write some code, do a query, update it, add the chart, deploy, blah blah blah blah. And that just like doesn't work because a lot of our customers have different data they want to look at within the product. So um we needed a way for us to give the customer a way to generate data based uh generate visuals based on the data in the application on demand. And so one of the things we experimented with which we mostly use internally but we might deploy to customers is we something called like I think it's called cube or something like that. I'll find the name of it, but essentially it you give it your database and then you tell it the context of each table and columns and from there you can pump it to any LLM and the LLM will write the right SQL query based on your your statements and from there the cube thing also creates the graphs and the charts or whatever and that can be based on your your CSS. So we haven't done anything like that but um that might be a similar thing where it's it's querying things correctly and it's not expensive. It's actually I believe is actually quite inexpensive the way you found it. So no, we we didn't I don't have that exact problem. >> No, that's I mean that's very useful and separate for me. So that's very good. So thank you. Thank you. >> Yeah. Um and I'd say there's like one funny thing. Um so one part of our design system is tables. So a lot of our apps just have like a table. Um and we said okay and when we were building this component of a table one of the things we said was like let's just add like AI search to the tables because all the tables that we have in our all of our apps have something like maybe up to 20 columns and so we hide a lot of them because not everyone needs all that data and so we wanted to let the customer do is say I would like to see all the leads that are from Facebook and assigned to Johnny and we want the the AI to you know like organize the table with only that data is available. And the part that is interesting is like when we implement this in the table, it's actually quite different than than using AI to look at your database. It's actually just like you tell the AI these are the these are the columns we have and this is what each of these columns mean. And then what you have is the user tells the AI what it wants. And the AI is actually a local LLM that's very small that is not connected to anything. it's actually disconnected from your it just has context but it's disconnected from your application or disconnected from your back end. And so the the AI the local AI is just translating the human sentence into a SQL query which then it runs against the API that returns a result. So there's no actual AI in the the applications on the the surface layer. And that's when I worked on that, it was very interesting because I was thinking to myself, there's like lots of ways to implement these LLMs at different levels that are very light and that are like very specific use cases and not necessarily complex and connected to your like you don't need that. And I find that most user experiences that are nice with AI is something like that where they make something a little more less annoying for the user user. And um that was the first time I I like I I was I came upon a scenario where we had to decide how to build AI feature. And that was interesting because it just was a small little LLM in the search bar that didn't know anything except a paragraph or a parag bunch of paragraphs you give it and and what the user wanted and just then gave a sequel to the API to hit it and return the results. And that was a very good AI implementation feature that didn't have anything complex. And so there's lots of ways to use these things just to make little things nice. I think across the experience, like filling out a form for example, like uh if you had like a funnel where you had to answer a bunch of questions, well, you could really just like have someone speak to it, like I want to create a marketing campaign that's targeting uh users that are likely to buy that came from China. Well, you say that's an LM. You can just tell the LM to to write a script for the form and it fills up a form and you get the users in. So, like there's lots of these little things that are not ultra high-tech that are really nice in the user experience that people are just not doing right now. >> Johnny, um, this has been fantastic. Um, I can see lots of people in the chat also just expressing their thanks. Um, it's been like incredible to get a really kind of like under the hood look at how you're doing things. So, thank you very much for um, >> spending that time with us. I'll stick the recording on YouTube. Um, and I will send everyone a quick email once it's once it's processed so you can check it out. But again, just thank you so much for your time, Johnny. Really, really appreciate it. >> Thanks, guys. It was fun. Talk to you later. Ciao. Bye.